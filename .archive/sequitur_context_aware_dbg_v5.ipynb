{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*to handle errors and diploidy*:\n",
    "    \n",
    "    1. find every unsequenced read\n",
    "    2. see if the unsequenced reads exist in the constructed sequence\n",
    "    3. construct sequences with unseen reads until they combine with a sequenced read in both directions\n",
    "    4. sort kmers by frequency of kmer occurence in all reads, more frequent kmers are more likely polyploidy and less frequent kmers are more likely error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequitur:\n",
    "    def __init__(self,read):\n",
    "        self.sequence = read.read\n",
    "        self.reads = [read]\n",
    "\n",
    "    def __repr__(self): return self.sequence\n",
    "\n",
    "    def __getitem__(self,key): return self.sequence[key]\n",
    "\n",
    "    def __eq__(self,other): return self.sequence == other\n",
    "\n",
    "    def __add__(self,other): \n",
    "        self.sequence += other\n",
    "        return self\n",
    "\n",
    "    def __radd__(self,other): \n",
    "        self.sequence = other + self.sequence\n",
    "        return self\n",
    "\n",
    "    def partition(self,root): return self.sequence.partition(root)\n",
    "\n",
    "    def rpartition(self,root): return self.sequence.rpartition(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The read is held by a leaf and sustains their uniqueness and ensures they're added to the sequence at most once.\n",
    "'''\n",
    "class Read:\n",
    "    def __init__(self,read):\n",
    "        self.read = read\n",
    "        self.is_sequenced = False\n",
    "\n",
    "    def __repr__(self): return self.read\n",
    "\n",
    "    def endswith(self,prefix): return self.read.endswith(prefix)\n",
    "    def startswith(self,suffix): return self.read.startswith(suffix)\n",
    "\n",
    "    def partition(self,root,dir):\n",
    "        if dir: return Node(self.read.rpartition(root)[0],dir), self.read.rpartition(root)[2]\n",
    "        else: return Node(self.read.partition(root)[2],dir), self.read.partition(root)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A leaf is the end point of a branch and can only the gained information. \n",
    "Every Trie must have a leaf for every read containing the root\n",
    "Leaves can become branches\n",
    "'''\n",
    "class Leaf:\n",
    "    def __init__(self,context,information,read):\n",
    "        self.context = context\n",
    "        if len(information) == 0: self.information = '$'\n",
    "        else: self.information = information\n",
    "        self.read = read\n",
    "\n",
    "    def __repr__(self): return str(self.information)\n",
    "    \n",
    "    def branch(self,context):\n",
    "        stalk = \"\"\n",
    "        i = 0\n",
    "        while i < min(len(context.stalk),len(self.context.stalk)) and context[i] == self.context[i]: \n",
    "            stalk += context[i]\n",
    "            i += 1\n",
    "        return stalk,self.context.stalk[len(stalk):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The node is a convenience class for accessing the branch\n",
    "'''\n",
    "class Node:\n",
    "    def __init__(self,stalk,dir=0):\n",
    "        if len(stalk) > 0: self.stalk = stalk\n",
    "        else: self.stalk = \"^\"\n",
    "        self.reversed = False\n",
    "        if dir: \n",
    "            self.stalk = ''.join(reversed(self.stalk))\n",
    "            self.reversed = True\n",
    "\n",
    "    def __eq__(self,other): #return self.stalk[0] == other.stalk[0]\n",
    "        i = 0\n",
    "        while i < min(len(self.stalk),len(other.stalk)):\n",
    "            if self.stalk[i] != other.stalk[i]: return False\n",
    "            i += 1\n",
    "        return True\n",
    "\n",
    "    def __hash__(self): return hash(self.stalk[0])\n",
    "\n",
    "    def __getitem__(self,index): return self.stalk[index]\n",
    "\n",
    "    def __repr__(self): return self.stalk\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.stalk == '^': return 0\n",
    "        else: return len(self.stalk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A branch has either a collection of branches or a collection of leaves\n",
    "Every branch must have at least 1 leaf\n",
    "'''\n",
    "class Branch:\n",
    "    def __init__(self,root,network,node=None,foothold=None):\n",
    "        self.branches = {}\n",
    "        self.leaves = {}\n",
    "        self.root = root\n",
    "        self.network = network\n",
    "        self.node = node\n",
    "        self.foothold = foothold\n",
    "\n",
    "    def grow(self,leaf,dir):\n",
    "        leaf.context.stalk = leaf.context[len(self.node):]\n",
    "        if leaf.context.stalk == '': leaf.context.stalk = '^'\n",
    "        if leaf.context in self.branches: self.branches[leaf.context].grow(leaf,dir)\n",
    "        else:\n",
    "            node = Node('',dir)\n",
    "            if leaf.context.stalk == '^':\n",
    "                if node in self.leaves: self.leaves[node] += [leaf]\n",
    "                else: self.leaves[node] = [leaf]\n",
    "            else:\n",
    "                stalk = ''\n",
    "                new_leaves = []\n",
    "                for l in list(self.leaves):\n",
    "                    s = leaf.branch(l)[0]\n",
    "                    if len(s) == 0: continue\n",
    "                    elif len(stalk) == 0 or len(s) < len(stalk): \n",
    "                        stalk = s\n",
    "                        new_leaves += [(l,self.leaves.pop(l))]\n",
    "                if len(stalk) > 0: node.stalk = stalk\n",
    "                self.branches[node] = Branch(self.root,self.network,node,self)\n",
    "                for l in new_leaves:\n",
    "                    l[0].stalk = l[0][len(node):]\n",
    "                    if l[0].stalk == '':\n",
    "                        l[0].stalk = '^'\n",
    "                        if l[0] in self.branches[node].leaves: self.branches[node].leaves[l[0]] += [l[1]]\n",
    "                        else: self.branches[node].leaves[l[0]] = [l[1]]\n",
    "                    else: self.branches[node].leaves[l[0]] = l[1]\n",
    "                leaf.context.stalk = leaf.context.stalk[len(node):]\n",
    "                if leaf.context.stalk == '':\n",
    "                    leaf.context.stalk = '^'\n",
    "                    if leaf.context in self.branches[node].leaves: self.branches[node].leaves[leaf.context] += [leaf]\n",
    "                    else: self.branches[node].leaves[leaf.context] = [leaf]\n",
    "                else: self.branches[node].leaves[leaf.context] = leaf\n",
    "\n",
    "    def climb(self,sequence,dir,stalk='',join_at=None):\n",
    "        if type(self) == Root:\n",
    "            stalk = self.root\n",
    "            if dir: context,_,_ = sequence.rpartition(stalk)\n",
    "            else: _,_,context = sequence.partition(stalk)\n",
    "        else:\n",
    "            if dir: \n",
    "                stalk = self.node.stalk + stalk\n",
    "                context,_,_ = sequence.rpartition(stalk)\n",
    "            else: \n",
    "                stalk += self.node.stalk \n",
    "                _,_,context = sequence.partition(stalk)\n",
    "        context = Node(context,dir)\n",
    "        if context in self.branches: return self.branches[context].climb(sequence,dir,stalk,join_at)\n",
    "        else: \n",
    "            if join_at and context in self.leaves and not context.stalk == '^' and join_at == self.leaves[context].read: return True\n",
    "            if context in self.leaves and not context.stalk == '^' and not self.leaves[context].read.is_sequenced and self.joins_with(self.leaves[context].read,sequence,not dir): \n",
    "                if dir: sequence += self.leaves[context].information\n",
    "                else: sequence = self.leaves[context].information + sequence\n",
    "                self.leaves[context].read.is_sequenced = True\n",
    "                sequence.reads += [self.leaves[context].read]\n",
    "                return sequence\n",
    "            else:\n",
    "                carat = Node('^',dir) \n",
    "                if carat in self.leaves:\n",
    "                    for leaf in self.leaves[carat]:\n",
    "                        if join_at and join_at == leaf.read: return True\n",
    "                        elif join_at: continue\n",
    "                        if leaf.read == sequence.reads[-1] or leaf.read.is_sequenced: continue\n",
    "                        if self.joins_with(leaf.read,sequence,not dir):\n",
    "                            if dir: sequence += leaf.information\n",
    "                            else: sequence = leaf.information + sequence\n",
    "                            leaf.read.is_sequenced = True\n",
    "                            sequence.reads += [leaf.read]\n",
    "                            return sequence\n",
    "                if self.foothold:\n",
    "                    return self.descend(sequence,dir,stalk[len(self.node.stalk):],join_at)\n",
    "                else: \n",
    "                    if join_at: return False\n",
    "                    if dir: return sequence + '$'\n",
    "                    else: return '^' + sequence\n",
    "    \n",
    "    def joins_with(self,read,sequence,dir):\n",
    "        if dir: # starts with\n",
    "            root = read.read[-self.network.k:]\n",
    "            ind = sequence.sequence.index(root)\n",
    "            suf = sequence.sequence[:ind+self.network.k]\n",
    "            b = read.endswith(suf)\n",
    "            return b\n",
    "        else: # ends with\n",
    "            root = read.read[:self.network.k]\n",
    "            ind = sequence.sequence.rindex(root)\n",
    "            pre = sequence.sequence[ind:]\n",
    "            b = read.startswith(pre)\n",
    "            return b\n",
    "        # if dir: return read.endswith(sequence.sequence[:sequence.sequence.index(read.read[-self.network.k:])+self.network.k])\n",
    "        # else: return read.startswith(sequence.sequence[sequence.sequence.rindex(read.read[:self.network.k]):])\n",
    "\n",
    "    def descend(self,sequence,dir,stalk,join_at):\n",
    "        carat = Node('',dir)\n",
    "        if carat in self.foothold.leaves:\n",
    "            for leaf in self.foothold.leaves[carat]:\n",
    "                if join_at and join_at == leaf.read: return True\n",
    "                elif join_at: continue\n",
    "                if leaf.read == sequence.reads[-1] or leaf.read.is_sequenced: continue\n",
    "                if self.joins_with(leaf.read,sequence,not dir):\n",
    "                    if dir: sequence += leaf.information\n",
    "                    else: sequence = leaf.information + sequence\n",
    "                    leaf.read.is_sequenced = True\n",
    "                    sequence.reads += [leaf.read]\n",
    "                    return sequence\n",
    "        if self.foothold:\n",
    "            return self.descend(sequence,dir,stalk[len(self.node.stalk):],join_at)\n",
    "        else: \n",
    "            if join_at: return False\n",
    "            if dir: return sequence + '$'\n",
    "            else: return '^' + sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A root is a branch with a connection to the network and a list of the reads it comprises of.\n",
    "It can have a collection of branches and leaves.\n",
    "'''\n",
    "class Root(Branch):\n",
    "    def __init__(self,root,network):\n",
    "        super().__init__(root,network)\n",
    "        self.reads = []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.root\n",
    "        \n",
    "    def add_read(self,read):\n",
    "        self.reads += [read]\n",
    "        self.network.reads += [read]\n",
    "\n",
    "    def grow(self,sequence,dir,join_at=None):\n",
    "        if len(self.reads) > 0: \n",
    "            if len(self.leaves) == 0: self.sprout(sequence,dir)\n",
    "            else: \n",
    "                read = self.reads.pop()\n",
    "                while read.is_sequenced and len(self.reads) > 0: read = self.reads.pop()\n",
    "                if not read.is_sequenced:\n",
    "                    context,information = read.partition(self.root,dir)\n",
    "                    if read.read == sequence: \n",
    "                        read.is_sequenced = True\n",
    "                        sequence.reads += [read]\n",
    "                    if context in self.leaves: \n",
    "                        node = Node(self.leaves[context].branch(context)[0])\n",
    "                        self.branches[node] = Branch(self.root,self.network,node)\n",
    "                        self.leaves[context].context = Node(self.leaves[context].context.stalk.partition(node.stalk)[2])\n",
    "                        if self.leaves[context].context.stalk == '^': \n",
    "                            if self.leaves[context].context in self.branches[node].leaves: self.branches[node].leaves[self.leaves[context].context] += [Leaf(self.leaves[context].context,self.leaves[context].information,self.leaves[context].read)]\n",
    "                            else: self.branches[node].leaves[self.leaves[context].context] = [Leaf(self.leaves[context].context,self.leaves[context].information,self.leaves[context].read)]\n",
    "                        else: self.branches[node].leaves[self.leaves[context].context] = Leaf(self.leaves[context].context,self.leaves[context].information,self.leaves[context].read)\n",
    "                        self.leaves.pop(context)\n",
    "                        context = Node(context.stalk.partition(node.stalk)[2])\n",
    "                        if context.stalk == '^': \n",
    "                            if context in self.branches[node].leaves: self.branches[node].leaves[context] += [Leaf(context,information,read)]\n",
    "                            else: self.branches[node].leaves[context] = [Leaf(context,information,read)]\n",
    "                        else: self.branches[node].leaves[context] = Leaf(context,information,read)\n",
    "                    else: \n",
    "                        if context.stalk == '^': self.leaves[context] = [Leaf(context,information,read)]\n",
    "                        else: self.leaves[context] = Leaf(context,information,read)\n",
    "            return self.grow(sequence,dir,join_at)\n",
    "        else: \n",
    "            if join_at: return self.climb(sequence,dir,'',join_at)\n",
    "            sequence = self.climb(sequence,dir)\n",
    "            if sequence[0] == '^' and not dir: return self.network.get_root(sequence,1).grow(sequence,1)\n",
    "            elif sequence[0] == '^' and sequence[-1] == '$': return sequence\n",
    "            return self.network.get_root(sequence,dir).grow(sequence,dir)\n",
    "\n",
    "    def sprout(self,sequence,dir):\n",
    "        read = self.reads.pop()\n",
    "        while read.is_sequenced and len(self.reads) > 0: read = self.reads.pop()\n",
    "        if not read.is_sequenced:\n",
    "            context,information = read.partition(self.root,dir)\n",
    "            if read.read == sequence: \n",
    "                read.is_sequenced = True\n",
    "                sequence.reads += [read]\n",
    "            if len(self.branches) > 0 and context in self.branches: \n",
    "                leaf = Leaf(context,information,read)\n",
    "                self.branches[context].grow(Leaf(context,information,read),dir)\n",
    "            else: \n",
    "                if context.stalk == '^': self.leaves[context] = [Leaf(context,information,read)]\n",
    "                else: self.leaves[context] = Leaf(context,information,read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RootNetwork:\n",
    "    def __init__(self,k):\n",
    "        self.roots = {}\n",
    "        self.reads = []\n",
    "        self.k = k\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        return self.roots[key]\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.roots\n",
    "\n",
    "    # dir = 1, context gain towards prefix\n",
    "    # dir = 0, context gain towards suffix\n",
    "    def build(self,sequence,dir=0): return self.get_root(sequence,dir).grow(sequence,dir)\n",
    "\n",
    "    def plant_trie(self,trie): self.roots[trie.root] = trie\n",
    "\n",
    "    def get_root(self,sequence,dir):\n",
    "        if dir: return self[sequence[-self.k:]]\n",
    "        else: return self[sequence[:self.k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(reads):\n",
    "    k = 3\n",
    "    r = RootNetwork(k)\n",
    "    R = {}\n",
    "    kmers = {}\n",
    "    for read in reads:\n",
    "        R[read] = Read(read)\n",
    "        for i in range(len(read)-k+1):\n",
    "            if read[i:i+k] in kmers: kmers[read[i:i+k]] += 1\n",
    "            else: kmers[read[i:i+k]] = 1\n",
    "            if read[i:i+k] not in r: r.plant_trie(Root(read[i:i+k],r))\n",
    "            r[read[i:i+k]].add_read(R[read])\n",
    "    return R,r,kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: you say hello world, i bellow go to hell\n",
      "Initus: you say hel [SUCCESS]\n",
      "Initus:  say hello wo [SUCCESS]\n",
      "Initus: lo world, i be [SUCCESS]\n",
      "Initus: ld, i bellow go t [SUCCESS]\n",
      "Initus: ow go to hell [SUCCESS]\n",
      "------------------------\n",
      "Accuracy: 100.0 %\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# you say hello world, i bellow go to hell\n",
    "successes = 0\n",
    "reads = ['you say hel',' say hello wo','lo world, i be','ld, i bellow go t','ow go to hell']\n",
    "print('Sequence: you say hello world, i bellow go to hell')\n",
    "for read in reads:\n",
    "    R,r,kmers = initialise(reads)\n",
    "    sequence = r.build(Sequitur(R[read]))[1:-1]\n",
    "    if sequence == 'you say hello world, i bellow go to hell':\n",
    "        successes += 1\n",
    "        print('Initus:',read,'[SUCCESS]')\n",
    "    else: print('Initus:',read,'[FAILURE] | result:',sequence)\n",
    "print('------------------------')\n",
    "print('Accuracy:',successes/len(reads)*100,'%')\n",
    "print('========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: she_sells_sea_shells_on_the_sea_shore\n",
      "Initus: she_sells_s [SUCCESS]\n",
      "Initus: lls_sea_shel [SUCCESS]\n",
      "Initus: ea_shells_o [SUCCESS]\n",
      "Initus: shells_on_the_s [SUCCESS]\n",
      "Initus: he_sea_s [SUCCESS]\n",
      "Initus: ea_shore [SUCCESS]\n",
      "------------------------\n",
      "Accuracy: 100.0 %\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# she_sells_sea_shells_on_the_sea_shore\n",
    "successes = 0\n",
    "reads = ['she_sells_s',\n",
    "               'lls_sea_shel',\n",
    "                    'ea_shells_o',\n",
    "                       'shells_on_the_s',\n",
    "                                  'he_sea_s',\n",
    "                                      'ea_shore']\n",
    "print('Sequence: she_sells_sea_shells_on_the_sea_shore')\n",
    "for read in reads:\n",
    "    R,r,kmers = initialise(reads)\n",
    "    sequence = r.build(Sequitur(R[read]))[1:-1]\n",
    "    if sequence == 'she_sells_sea_shells_on_the_sea_shore':\n",
    "        successes += 1\n",
    "        print('Initus:',read,'[SUCCESS]')\n",
    "    else: print('Initus:',read,'[FAILURE] | result:',sequence)\n",
    "print('------------------------')\n",
    "print('Accuracy:',successes/len(reads)*100,'%')\n",
    "print('========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ ] recursive approach: build in the selected direction until the other end or until you reach the same read (indicating a circular sequence or an incorrect build)\n",
    "\n",
    "[ ] probabilistic approach: build from unique regions (areas of low kmer coverage) towards common of repeated regions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build(*sequence*,*mode*=0,*dir*=None):\n",
    "    if *mode*==0:\n",
    "        if there is no *dir*: \n",
    "            find the longest overlap that has not already been added to *sequence* and store it as *candidate*\n",
    "            add *candidate* to *sequence* and store the direction to *dir*\n",
    "        else:\n",
    "            find the longest overlap on the *dir* end that has not already been added to *sequence* and store it as *candidate*\n",
    "            add *candidate* to *sequence*\n",
    "        build(*sequence*,1,*dir*)\n",
    "    else:\n",
    "        find the longest overlap on the *dir* end that is has not already been added to *sequence* and store it as *candidate*\n",
    "        if there is no *candidate*: build(*sequence*,0,*dir*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better\n",
      "Initus: betty_bought_butter_th [FAILURE] | result: tter_the_butter_was_bitter_betty_bought_butter_th\n",
      "Initus: tter_the_butter_was_ [FAILURE] | result: tty_bought_better_butter_to_make_the_butter_was_bitter_betty_bought_butter_the_butter_was_\n",
      "Initus: he_butter_was_bitter_ [FAILURE] | result: as_bitter_betty_bought_butter_the_butter_was_bitter_\n",
      "Initus: as_bitter_betty_bought [FAILURE] | result: betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_\n",
      "Initus: tty_bought_better_butter_t [FAILURE] | result: betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_\n",
      "Initus: r_butter_to_make_the_ [FAILURE] | result: betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_\n",
      "Initus: r_to_make_the_bitt [SUCCESS]\n",
      "Initus: ke_the_bitter_butter_better [SUCCESS]\n",
      "------------------------\n",
      "Accuracy: 25.0 %\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "successes = 0\n",
    "# longest repeat: betty_bought_b (14)\n",
    "# shortest overlap: tter_th (7)\n",
    "reads = ['betty_bought_butter_th',\n",
    "                        'tter_the_butter_was_',\n",
    "                              'he_butter_was_bitter_',\n",
    "                                         'as_bitter_betty_bought',\n",
    "                                                     'tty_bought_better_butter_t',\n",
    "                                                                     'r_butter_to_make_the_',\n",
    "                                                                            'r_to_make_the_bitt',\n",
    "                                                                                   'ke_the_bitter_butter_better']\n",
    "print('Sequence: betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better')\n",
    "for read in reads:\n",
    "    R,r,kmers = initialise(reads)\n",
    "    sequence = r.build(Sequitur(R[read]))[1:-1]\n",
    "    if sequence == 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better':\n",
    "        successes += 1\n",
    "        print('Initus:',read,'[SUCCESS]')\n",
    "    else: print('Initus:',read,'[FAILURE] | result:',sequence)\n",
    "print('------------------------')\n",
    "print('Accuracy:',successes/len(reads)*100,'%')\n",
    "print('========================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBruijn Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import random\n",
    "\n",
    "# Generate all possible k-mers of length k\n",
    "def kmers(k,seed=None):\n",
    "    random.seed(seed)\n",
    "    perms = list(permutations('ATCG', k))\n",
    "    random.shuffle(perms)\n",
    "    return [''.join(p) for p in perms]\n",
    "\n",
    "# Generate a sequence of length n where each k-mer appears exactly once\n",
    "def sequence(k, n,seed=None):\n",
    "    seq = \"\"\n",
    "    for kmer in kmers(k,seed):\n",
    "        seq += kmer\n",
    "    return seq[:n]\n",
    "\n",
    "def generate_reads(seq, k, min_overlap, max_overlap,seed=None):\n",
    "    random.seed(seed)\n",
    "    reads = []\n",
    "    for i in range(0, len(seq)-k+1):\n",
    "        # Generate a random overlap within the specified range\n",
    "        overlap = random.randint(min_overlap, max_overlap)\n",
    "        start = i\n",
    "        end = i + k + overlap\n",
    "        reads.append(seq[start:end])\n",
    "    return reads\n",
    "    \n",
    "class DeBruijnGraph:\n",
    "    def __init__(self,reads,k):\n",
    "        self.nodes = {}\n",
    "        self.unexplored = []\n",
    "        for read in reads:\n",
    "            for i in range(len(read)-k+1):\n",
    "                if read[i:i+k-1] in self.nodes: \n",
    "                    self.nodes[read[i:i+k-1]] += [(read[i:i+k-1],read[i+1:i+k])]\n",
    "                else: \n",
    "                    self.nodes[read[i:i+k-1]] = [(read[i:i+k-1],read[i+1:i+k])]\n",
    "                self.unexplored += [(read[i:i+k-1],read[i+1:i+k])]\n",
    "    def __repr__(self): return self.nodes\n",
    "    def traverse(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads = ['betty_bought_butter_th',\n",
    "#                         'tter_the_butter_was_',\n",
    "#                               'he_butter_was_bitter_',\n",
    "#                                          'as_bitter_betty_bought',\n",
    "#                                                      'tty_bought_better_butter_t',\n",
    "#                                                                      'r_butter_to_make_the_',\n",
    "#                                                                             'r_to_make_the_bitt',\n",
    "#                                                                                    'ke_the_bitter_butter_better']\n",
    "seed=1\n",
    "# seq=sequence(3, 12, seed)\n",
    "seq='she_sells_sea_shells_on_the_sea_shore'\n",
    "# reads = generate_reads(seq, 3, 0, 0, seed)\n",
    "reads = ['she_sells_s',\n",
    "               'lls_sea_shel',\n",
    "                    'ea_shells_o',\n",
    "                       'shells_on_the_s',\n",
    "                                  'he_sea_s',\n",
    "                                      'ea_shore']\n",
    "dbg = DeBruijnGraph(reads,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she_sells_sea_shells_on_the_sea_shore'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['she_sells_s',\n",
       " 'lls_sea_shel',\n",
       " 'ea_shells_o',\n",
       " 'shells_on_the_s',\n",
       " 'he_sea_s',\n",
       " 'ea_shore']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'she_sel': [('she_sel', 'he_sell')],\n",
       " 'he_sell': [('he_sell', 'e_sells')],\n",
       " 'e_sells': [('e_sells', '_sells_')],\n",
       " '_sells_': [('_sells_', 'sells_s')],\n",
       " 'lls_sea': [('lls_sea', 'ls_sea_')],\n",
       " 'ls_sea_': [('ls_sea_', 's_sea_s')],\n",
       " 's_sea_s': [('s_sea_s', '_sea_sh')],\n",
       " '_sea_sh': [('_sea_sh', 'sea_she')],\n",
       " 'sea_she': [('sea_she', 'ea_shel')],\n",
       " 'ea_shel': [('ea_shel', 'a_shell')],\n",
       " 'a_shell': [('a_shell', '_shells')],\n",
       " '_shells': [('_shells', 'shells_')],\n",
       " 'shells_': [('shells_', 'hells_o'), ('shells_', 'hells_o')],\n",
       " 'hells_o': [('hells_o', 'ells_on')],\n",
       " 'ells_on': [('ells_on', 'lls_on_')],\n",
       " 'lls_on_': [('lls_on_', 'ls_on_t')],\n",
       " 'ls_on_t': [('ls_on_t', 's_on_th')],\n",
       " 's_on_th': [('s_on_th', '_on_the')],\n",
       " '_on_the': [('_on_the', 'on_the_')],\n",
       " 'on_the_': [('on_the_', 'n_the_s')],\n",
       " 'he_sea_': [('he_sea_', 'e_sea_s')],\n",
       " 'ea_shor': [('ea_shor', 'a_shore')]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she_sel', 'he_sell'),\n",
       " ('he_sell', 'e_sells'),\n",
       " ('e_sells', '_sells_'),\n",
       " ('_sells_', 'sells_s'),\n",
       " ('lls_sea', 'ls_sea_'),\n",
       " ('ls_sea_', 's_sea_s'),\n",
       " ('s_sea_s', '_sea_sh'),\n",
       " ('_sea_sh', 'sea_she'),\n",
       " ('sea_she', 'ea_shel'),\n",
       " ('ea_shel', 'a_shell'),\n",
       " ('a_shell', '_shells'),\n",
       " ('_shells', 'shells_'),\n",
       " ('shells_', 'hells_o'),\n",
       " ('shells_', 'hells_o'),\n",
       " ('hells_o', 'ells_on'),\n",
       " ('ells_on', 'lls_on_'),\n",
       " ('lls_on_', 'ls_on_t'),\n",
       " ('ls_on_t', 's_on_th'),\n",
       " ('s_on_th', '_on_the'),\n",
       " ('_on_the', 'on_the_'),\n",
       " ('on_the_', 'n_the_s'),\n",
       " ('he_sea_', 'e_sea_s'),\n",
       " ('ea_shor', 'a_shore')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbg.unexplored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lls_sea', 'ls_sea_')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('on_the_', 'n_the_s'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('e_sells', '_sells_'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('lls_sea', 'ls_sea_'), ('ls_sea_', 's_sea_s'), ('s_sea_s', '_sea_sh'), ('_sea_sh', 'sea_she'), ('sea_she', 'ea_shel'), ('ea_shel', 'a_shell'), ('a_shell', '_shells'), ('_shells', 'shells_'), ('shells_', 'hells_o'), ('hells_o', 'ells_on'), ('ells_on', 'lls_on_'), ('lls_on_', 'ls_on_t'), ('ls_on_t', 's_on_th'), ('s_on_th', '_on_the'), ('_on_the', 'on_the_'), ('on_the_', 'n_the_s')]\n",
      "[('e_sells', '_sells_')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('_sells_', 'sells_s'), ('shells_', 'hells_o'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('e_sells', '_sells_'), ('_sells_', 'sells_s')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('shells_', 'hells_o'), ('he_sea_', 'e_sea_s'), ('ea_shor', 'a_shore')]\n",
      "\n",
      "[('e_sells', '_sells_'), ('_sells_', 'sells_s')]\n",
      "[('ea_shor', 'a_shore')]\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells'), ('shells_', 'hells_o'), ('he_sea_', 'e_sea_s')]\n",
      "\n",
      "[('ea_shor', 'a_shore')]\n",
      "[('she_sel', 'he_sell')]\n",
      "[('he_sell', 'e_sells'), ('shells_', 'hells_o'), ('he_sea_', 'e_sea_s')]\n",
      "\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells')]\n",
      "[('shells_', 'hells_o'), ('he_sea_', 'e_sea_s')]\n",
      "\n",
      "[('she_sel', 'he_sell'), ('he_sell', 'e_sells')]\n",
      "[('shells_', 'hells_o')]\n",
      "[('he_sea_', 'e_sea_s')]\n",
      "\n",
      "[('shells_', 'hells_o')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import copy\n",
    "\n",
    "random.seed(seed)\n",
    "nodes = copy.deepcopy(dbg.nodes)\n",
    "unexplored = dbg.unexplored.copy()\n",
    "node = random.choice(unexplored)\n",
    "unexplored.remove(node)\n",
    "nodes[node[0]].remove(node)\n",
    "cycles = {}\n",
    "cycle = [node]\n",
    "while len(unexplored):\n",
    "    print(cycle)\n",
    "    print(unexplored)\n",
    "    print()\n",
    "    if node[1] in nodes and len(nodes[node[1]]):\n",
    "        node = nodes[node[1]].pop(random.randint(0,len(nodes[node[1]])-1))\n",
    "        unexplored.remove(node)\n",
    "        cycle += [node]\n",
    "    else:\n",
    "        print(cycle)\n",
    "        if cycle[0][0] in cycles: cycles[cycle[0][0]] += cycle\n",
    "        else: cycles[cycle[0][0]] = cycle\n",
    "        node = random.choice(unexplored)\n",
    "        unexplored.remove(node)\n",
    "        nodes[node[0]].remove(node)\n",
    "        cycle = [node]\n",
    "if cycle[0][0] in cycles: cycles[cycle[0][0]] += cycle\n",
    "else: cycles[cycle[0][0]] = cycle\n",
    "# if cycle[-1][1] in cycles:\n",
    "#     cycles[cycle[0][0]] += cycles[cycle[-1][1]]\n",
    "#     if len(cycles[cycle[-1][1]]) == 0:cycles.pop(cycle[-1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lls_sea': [('lls_sea', 'ls_sea_'),\n",
       "  ('ls_sea_', 's_sea_s'),\n",
       "  ('s_sea_s', '_sea_sh'),\n",
       "  ('_sea_sh', 'sea_she'),\n",
       "  ('sea_she', 'ea_shel'),\n",
       "  ('ea_shel', 'a_shell'),\n",
       "  ('a_shell', '_shells'),\n",
       "  ('_shells', 'shells_'),\n",
       "  ('shells_', 'hells_o'),\n",
       "  ('hells_o', 'ells_on'),\n",
       "  ('ells_on', 'lls_on_'),\n",
       "  ('lls_on_', 'ls_on_t'),\n",
       "  ('ls_on_t', 's_on_th'),\n",
       "  ('s_on_th', '_on_the'),\n",
       "  ('_on_the', 'on_the_'),\n",
       "  ('on_the_', 'n_the_s')],\n",
       " 'e_sells': [('e_sells', '_sells_'), ('_sells_', 'sells_s')],\n",
       " 'ea_shor': [('ea_shor', 'a_shore')],\n",
       " 'she_sel': [('she_sel', 'he_sell'), ('he_sell', 'e_sells')],\n",
       " 'shells_': [('shells_', 'hells_o')],\n",
       " 'he_sea_': [('he_sea_', 'e_sea_s')]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls_sells_on_the_sea_shells_s\n",
      "esea_shells_o\n",
      "h_s\n",
      "sore\n",
      "se_el\n",
      "_ho\n",
      "e_s_s\n"
     ]
    }
   ],
   "source": [
    "for a in cycles:\n",
    "    s = cycles[a][0][0][0]\n",
    "    for b in cycles[a]:\n",
    "        s += b[0][-1]\n",
    "    s += b[1][-1]\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import suffix_tree #import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = ['betty_bought_butter_th',\n",
    "                        'tter_the_butter_was_',\n",
    "                              'he_butter_was_bitter_',\n",
    "                                         'as_bitter_betty_bought',\n",
    "                                                     'tty_bought_better_butter_t',\n",
    "                                                                     'r_butter_to_make_the_',\n",
    "                                                                            'r_to_make_the_bitt',\n",
    "                                                                                   'ke_the_bitter_butter_better']\n",
    "tree = Tree()\n",
    "i = 0\n",
    "for read in reads:\n",
    "    tree.add(i, read)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : _ w a s _ $\n",
      "2 : _ w a s _ b i t t e r _ $\n"
     ]
    }
   ],
   "source": [
    "for id_, path in tree.find_all(\"_was_\"):\n",
    "    print(id_, \":\", str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 14 h e _ b u t t e r _ w a s _\n",
      "3 10 t t y _ b o u g h t\n",
      "4 8 _ b u t t e r _\n",
      "5 8 _ b u t t e r _\n",
      "6 8 _ b u t t e r _\n",
      "7 5 t t e r _\n",
      "8 2 t t\n"
     ]
    }
   ],
   "source": [
    "for k, lk, path in tree.common_substrings():\n",
    "    print(k, lk, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 _ b e t t e r\n",
      "2 _ b i t t e r _ b\n",
      "2 a s _ b i t t e r _\n",
      "2 b e t t y _ b o u g h t\n",
      "2 h e _ b u t t e r _ w a s _\n",
      "2 k e _ t h e _ b i t t\n",
      "2 r _ b u t t e r _ t\n",
      "2 r _ t o _ m a k e _ t h e _\n",
      "2 t t e r _ b e t t\n",
      "2 t t e r _ b u t t e r _\n",
      "2 t t e r _ t h\n",
      "2 t t y _ b o u g h t _ b\n",
      "3 _ b e t t\n",
      "3 _ b i t t e r _\n",
      "3 _ b u t t e r _ t\n",
      "3 _ t h e _ b\n",
      "3 a s _\n",
      "3 k e _ t h e _\n",
      "3 r _ b u t t e r _\n",
      "3 t t e r _ b\n",
      "3 t t y _ b o u g h t\n",
      "4 _ b i t t\n",
      "4 _ t h e _\n",
      "4 b e t t\n",
      "4 h e _ b\n",
      "4 r _ b\n",
      "4 t t e r _ t\n",
      "5 _ t h\n",
      "5 a\n",
      "5 e _\n",
      "5 h e _\n",
      "5 o\n",
      "5 r _ t\n",
      "6 _ b u t t e r _\n",
      "6 _ t\n",
      "7 t t e r\n",
      "7 t t e r _\n",
      "7 u\n",
      "8 _\n",
      "8 _ b\n",
      "8 b\n",
      "8 e\n",
      "8 h\n",
      "8 r\n",
      "8 r _\n",
      "8 t\n",
      "8 t t\n"
     ]
    }
   ],
   "source": [
    "for C, path in sorted(tree.maximal_repeats()):\n",
    "    print(C, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutant-no_snps.gff-24960/1\n",
      "Seq('AATGTTGTCACTTGGATTCAAATGACATTTTAAATCTAATTATTCATGAATCGA...TTT')\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "for seq_record in SeqIO.parse(\"data/mutant_R1.fastq\", \"fastq\"):\n",
    "   print(seq_record.id)\n",
    "   print(repr(seq_record.seq))\n",
    "   print(len(seq_record))\n",
    "   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "38731f125b301d8f0df7c54051f2a9a4c898c9398d16ef376d9fb7d661d33405"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
