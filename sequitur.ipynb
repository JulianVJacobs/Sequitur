{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "from typing import Union\n",
    "from scipy.sparse import coo_matrix, vstack, hstack, save_npz, load_npz # pip install scipy\n",
    "from jellyfish import damerau_levenshtein_distance # pip install jellyfish\n",
    "import numpy as np # pip install numpy\n",
    "from math import ceil, fmod\n",
    "import networkx as nx # pip install networkx\n",
    "import pylcs # pip install pylcs\n",
    "from Bio import SeqIO # pip install Bio\n",
    "import gzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are utility functions for generating random sequences and for generating reads of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_reads(sequence,min_subseq_len,max_subseq_len,min_overlap,max_overlap,min_coverage=None,circularise=False,seed=None,shuffle=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION \n",
    "        Utility function that chops a sequence into several reads with bounded random lengths that \n",
    "        have a bounded random overlap\n",
    "    INPUT\n",
    "        sequence       | a sequence of characters that will be divided into overlapping subsequences\n",
    "        min_subseq_len | the shortest length a subsequence can have\n",
    "        max_subseq_len | the longest length a subsequence can have\n",
    "        min_overlap    | the shortest overlap two subsequences can share\n",
    "        max_overlap    | the longest overlap two subsequences can share\n",
    "        circularize    | boolean indicating whether to add a random amount of the end of the sequence\n",
    "                    | to the beginning and vice versa\n",
    "        seed           | random seed for the random function for reproducibility\n",
    "    OUTPUT\n",
    "        A list of overlapping reads of random bounded size which share a bounded random amount of\n",
    "        overlap\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    if circularise: sequence = sequence[-random.randint(min_overlap,max_overlap):] + sequence + sequence[:random.randint(min_overlap,max_overlap)]\n",
    "    reads = []\n",
    "    while 1: \n",
    "        start = 0\n",
    "        end = random.randint(min_subseq_len,max_subseq_len)\n",
    "        reads += [sequence[start:end]]\n",
    "        while end < len(sequence):\n",
    "            start = random.randint(end-max_overlap,end-min_overlap)\n",
    "            if (len(sequence) - start)/max_subseq_len < 2:\n",
    "                if (len(sequence) - start)/max_subseq_len < 1:\n",
    "                    end = len(sequence)\n",
    "                else:\n",
    "                    a = 0\n",
    "                    while (len(sequence) - start)/(min_subseq_len+a) > 2: a+=1\n",
    "                    end = random.randint(start+min_subseq_len+a,start+max_subseq_len) \n",
    "            else: end = random.randint(start+min_subseq_len,start+max_subseq_len) \n",
    "            reads += [sequence[start:end]]\n",
    "        if min_coverage is None or len(set(reads))*(sum(len(read) for read in set(reads))/len(set(reads)))/len(sequence) >= min_coverage:\n",
    "            if not shuffle: return reads\n",
    "            reads_ = reads[:]\n",
    "            random.shuffle(reads_)\n",
    "            return reads_, list(reads_.index(read) for read in reads)\n",
    "\n",
    "\n",
    "def generate_genome_sequence(n,palindrome=False,seed=None):\n",
    "    \"\"\"\n",
    "    DESCRIPTION \n",
    "        Utility function that creates a random sequence containing only the letters A, T, G, and C\n",
    "    INPUT\n",
    "        n          | the length of the sequence\n",
    "        palindrome | a boolean indicating whether the sequence must be a palidrome or not\n",
    "        seed       | random seed for the random function for reproducibility\n",
    "    OUTPUT\n",
    "        A random sequence of length n\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    random.seed(seed)\n",
    "    nucleotides = {1:'A',2:'C',3:'G',4:'T'}\n",
    "    seq = ''\n",
    "    if palindrome: n = ceil(n/2)\n",
    "    for _ in range(n):\n",
    "        seq += nucleotides[random.randint(1,4)]\n",
    "    if palindrome: seq += ''.join(reversed(seq[:int(n-fmod(n,2))]))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Bruijn Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation for a De Bruijn Graph assembler. In reality, many statistical, optimisation, and computation techniques are implemented to improve the efficiency and quality of input. Here we use a basic technique to compare to our basic technique. The idea being that with the addition of similar developments, the novel technique presented here might be able to at least match or perhaps surpass this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_overlap(reads: list) -> int:\n",
    "    overlaps = []\n",
    "    for read in reads:\n",
    "        overlaps += pylcs.lcs2_of_list(read, list(set(reads).symmetric_difference([read])))\n",
    "    return min(overlaps),max(overlaps)\n",
    "\n",
    "def create_de_bruijn_graph(k: int, sequences: list,do_time: bool = False) -> Union[nx.Graph,tuple]:\n",
    "    \"\"\"\n",
    "    Create a de Bruijn graph from a set of DNA sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - k (int): k-mer size\n",
    "    - sequences (list): List of DNA sequences\n",
    "    - do_time (bool): should the function be time of not (default False)\n",
    "    \n",
    "    Returns:\n",
    "    - Union[nx.Graph,(nx.Graph,float)]: De Bruijn graph or a tuple of the De Bruijn graph and the time\n",
    "                                        to create it.\n",
    "    \"\"\"\n",
    "    if do_time: start_time = time.time()\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for i in range(len(sequence) - k + 1):\n",
    "            kmer = sequence[i:i+k]\n",
    "            prefix = kmer[:-1]\n",
    "            suffix = kmer[1:]\n",
    "            \n",
    "            if not graph.has_edge(prefix, suffix):\n",
    "                graph.add_edge(prefix, suffix, weight=1)\n",
    "            else:\n",
    "                graph[prefix][suffix]['weight'] += 1\n",
    "\n",
    "    if do_time: return graph, time.time() - start_time\n",
    "    return graph\n",
    "\n",
    "def eulerian_path(graph: nx.DiGraph,do_time: bool = False) -> Union[str,tuple]:\n",
    "    \"\"\"\n",
    "    Find an Eulerian path in the given graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph (nx.Graph): De Bruijn graph\n",
    "    - do_time (bool): should the function be time of not (default False)\n",
    "    \n",
    "    Returns:\n",
    "    - Union[str,(str,float)]: string describing the eulerian path or a tuple of this \n",
    "                              string and the execution time.\n",
    "    \"\"\"\n",
    "    if do_time: start_time = time.time()\n",
    "    path = []\n",
    "    \n",
    "    if nx.has_eulerian_path(graph):\n",
    "        for node in nx.eulerian_path(graph):\n",
    "            if len(path): path.append(node[0][-1])\n",
    "            else: path.append(node[0])\n",
    "        \n",
    "        if len(node[1]): path.append(node[1][-1])\n",
    "        else: path.append(node[1])\n",
    "        \n",
    "        if do_time: return ''.join(path), time.time() - start_time\n",
    "        return ''.join(path)\n",
    "    else: \n",
    "        if time: return '', time.time() - start_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods necessary for implementing the Sequitur assembly technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_damerau_levenshtein_distance(read: str,overlap: str) -> float:\n",
    "    \"\"\"\n",
    "    Find the Damerau-Levenshtein edit distance of two strings normalised to the length\n",
    "    of the shorter string. This normalisation is because we want to path prefixes to\n",
    "    suffixes and this means that in general we will be comparing a full string to a\n",
    "    portion of another string.\n",
    "    \n",
    "    Parameters:\n",
    "    - read (str): string for comparison, usually the longer string \n",
    "    - overlap (str): string for comparison, usually the shorter string\n",
    "    \n",
    "    Returns:\n",
    "    - float: the normalised Demarau-Levenshtein edit distance of the input strings\n",
    "    \"\"\"\n",
    "    return damerau_levenshtein_distance(read.__str__()[:min(len(overlap),len(read))],overlap.__str__()[:min(len(overlap),len(read))])/min(len(overlap),len(read))\n",
    "\n",
    "def build_suffix_array(reads: list, min_suf_len: int = 3,do_time: bool = False) -> tuple:\n",
    "    if do_time: start = time.time()\n",
    "    suf_arr = []\n",
    "    for read in reads:\n",
    "        read += '$' + str(reads.index(read))\n",
    "        for i in range(len(read)-min_suf_len-1):\n",
    "            # if len(read[i:]) < min_suf_len + 2: continue \n",
    "            suf_arr += [read[i:]]\n",
    "    suf_arr.sort()\n",
    "    suf_arr_ind = []\n",
    "    for s in range(len(suf_arr)):\n",
    "        suf_arr_ind += [int(suf_arr[s].split('$')[-1].__str__())]\n",
    "        suf_arr[s] = suf_arr[s][:suf_arr[s].find('$')+1]\n",
    "    if do_time: return suf_arr, suf_arr_ind, time.time() - start\n",
    "    return suf_arr,suf_arr_ind\n",
    "\n",
    "def create_bipartite_adjacency_matrix(reads: list, suf_arr: list = None, suf_arr_ind: list = None, do_time: bool = False,max_diff: float = 0.25, min_suf_len: int = 3) -> dict:\n",
    "    if do_time: start = time.time()\n",
    "    elif suf_arr is None or suf_arr_ind is None: suf_arr,suf_arr_ind = build_suffix_array(reads,min_suf_len=min_suf_len)\n",
    "    reads_map = dict(zip(reads,list(range(len(reads)))))\n",
    "    B = {}\n",
    "    for read in reads:\n",
    "        for j in range(min_suf_len + 1):\n",
    "            i = suf_arr.index(read[j:]+'$') - 1\n",
    "            while normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) <= 0.5:\n",
    "                if not reads[suf_arr_ind[i]] == read and \\\n",
    "                   normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) < max_diff and \\\n",
    "                   read.startswith(suf_arr[i][:-1]):\n",
    "                    if (reads_map[reads[suf_arr_ind[i]]],reads_map[read]) not in B: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = len(suf_arr[i][:-1])\n",
    "                    else: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = max(len(suf_arr[i][:-1]),B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])])\n",
    "                i -= 1\n",
    "    if do_time: return B, time.time() - start\n",
    "    return B\n",
    "\n",
    "def move_col(B: coo_matrix, cols: dict) -> None:\n",
    "    for c in range(len(B.col)):\n",
    "        B.col[c] = cols[B.col[c]]\n",
    "            \n",
    "def move_row(B: coo_matrix,rows: dict) -> None:\n",
    "    for r in range(len(B.row)):\n",
    "        B.row[r] = rows[B.row[r]]\n",
    "\n",
    "def find_lower_diagonal_path(B: coo_matrix,reads_map: dict,cols: list,rows: list,do_time: bool = False) -> tuple:\n",
    "    if do_time: start = time.time()\n",
    "    argpen = lambda l: np.argpartition(l,-2)[-2]\n",
    "\n",
    "    new_cols = cols[:]\n",
    "    if B.sum(axis=0).min() == 0: new_cols = list(c for c in new_cols if c not in [new_cols[B.sum(axis=0).argmin()]]) + [new_cols[B.sum(axis=0).argmin()]]\n",
    "    if B.sum(axis=1).min() == 0: \n",
    "        if B.sum(axis=1).argmin() == B.sum(axis=0).argmin():\n",
    "            new_cols = [new_cols[-1]] + list(c for c in new_cols[:-1] if c not in [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]) + [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]\n",
    "        else: new_cols = [rows[B.sum(axis=1).argmin()]] + list(c for c in new_cols if c not in [rows[B.sum(axis=1).argmin()]])\n",
    "\n",
    "    cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "    move_col(B,cols_map)\n",
    "    cols = new_cols\n",
    "\n",
    "    new_rows = cols[:]\n",
    "    rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "    move_row(B,rows_map)\n",
    "    rows = new_rows\n",
    "\n",
    "    i,j,k = len(rows), len(cols) - 1, B.sum(axis=1).argmin() if B.sum(axis=1).min() == 0 else None\n",
    "\n",
    "    while j > (k if B.sum(axis=1).min() == 0 else 0):\n",
    "        if k is not None and B.getrow(rows.index(cols[j])).argmax() == k: \n",
    "            cols_,c_ = [], 0\n",
    "\n",
    "            while j + c_ + 1 < len(rows):\n",
    "                c_ += 1\n",
    "                if len(B.getrow(j+c_).nonzero()[1]) > 1:\n",
    "                    cols_ = np.argpartition(B.getrow(j+c_).toarray().flatten(),-2)[::-1][:2]\n",
    "                    if cols[cols_[1]] in cols[:j] and B.getcol(cols_[1]).argmax() == j+c_: break\n",
    "            \n",
    "            if j + c_ + 1 == len(cols): new_cols = cols[:k+1] + cols[j:] + cols[k+1:j]\n",
    "            else: new_cols = cols[:k+1] + cols[j:j+c_] + list(c for c in cols[k+1:j] if c not in [cols[min(cols_)]]) + [cols[min(cols_)]] + cols[j+c_:]\n",
    "            cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "            move_col(B,cols_map)\n",
    "            cols = new_cols\n",
    "\n",
    "            if j + c_ + 1 == len(rows): new_rows = cols[:]\n",
    "            else: new_rows = cols[:k+c_+1] + list(r for r in rows[k:j+c_] if r not in cols[:k+c_+1] + cols[j+c_:]) + cols[j+c_:]\n",
    "            rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "            move_row(B,rows_map)\n",
    "            rows = new_rows\n",
    "\n",
    "            i,j,k = j + c_ + 1, j + c_, k + c_\n",
    "        else:\n",
    "            cmax = B.getrow(rows.index(cols[j])).argmax()\n",
    "            if len(B.getrow(rows.index(cols[j])).nonzero()[1]) > 1:\n",
    "                cpen = argpen(B.getrow(rows.index(cols[j])).toarray().flatten()) \n",
    "                if cmax > j: \n",
    "                    if len(B.getrow(cmax+1).nonzero()[1]) > 1 and \\\n",
    "                    B.getrow(cmax+1).getcol(argpen(B.getrow(cmax+1).toarray().flatten())).data[0] >=  B.getrow(rows.index(cols[j])).getcol(cpen).data[0]: \n",
    "                        crange = [argpen(B.getrow(cmax).toarray().flatten()),cmax]\n",
    "                    else: crange = [cpen]\n",
    "                else: crange = [cmax]\n",
    "            else: crange = [cmax]\n",
    "            while crange[0] > j:\n",
    "                if len(B.getrow(crange[0]).nonzero()[1]) > 1:\n",
    "                    crange = [argpen(B.getrow(crange[0]).toarray().flatten())] + crange\n",
    "                else:\n",
    "                    crange = [B.getrow(crange[0]).argmax()] + crange\n",
    "                if crange[0] == j: crange = [B.getrow(crange[1]).argmax()] + crange[1:]\n",
    "\n",
    "            new_cols = list(c for c in cols[:j] if c not in list(cols[cr] for cr in crange)) + list(cols[cr] for cr in crange) + list(c for c in cols[j:] if c not in list(cols[cr] for cr in crange))\n",
    "            cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "            move_col(B,cols_map)\n",
    "            cols = new_cols\n",
    "\n",
    "            new_rows = list(r for r in rows[:i] if r not in cols[j:]) + cols[j:]\n",
    "            rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "            move_row(B,rows_map)\n",
    "            rows = new_rows\n",
    "        j -= 1\n",
    "        i -= 1\n",
    "\n",
    "    seq = ''\n",
    "    for s,d in zip(list(reads_map[k] for k in rows)[:-1],B.diagonal(-1)):\n",
    "        seq += s[:-d]\n",
    "    seq += list(reads_map[k] for k in rows)[-1]\n",
    "    if do_time: return seq, time.time() - start\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 1\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_1_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "    reads = ['betty_bought_butter_th',\n",
    "                            'tter_the_butter_was_',\n",
    "                                'he_butter_was_bitter_',\n",
    "                                        'as_bitter_betty_bought',\n",
    "                                                    'tty_bought_better_butter_t',\n",
    "                                                        'ught_better_butter_to',\n",
    "                                                                    'r_butter_to_make_the_',\n",
    "                                                                                'ke_the_bitter_butter_better']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_1_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "    reads = ['betty_bought_butter_th',\n",
    "                            'tter_the_butter_was_',\n",
    "                                'he_butter_was_bitter_',\n",
    "                                        'as_bitter_betty_bought',\n",
    "                                                    'tty_bought_better_butter_t',\n",
    "                                                        'ught_better_butter_to',\n",
    "                                                                    'r_butter_to_make_the_',\n",
    "                                                                                'ke_the_bitter_butter_better']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 2\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_2_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'you say hello world, i bellow go to hell'\n",
    "    reads = ['you say hel',\n",
    "                ' say hello wo',\n",
    "                        'lo world, i be',\n",
    "                            'ld, i bellow go t',\n",
    "                                        'ow go to hell']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_2_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'you say hello world, i bellow go to hell'\n",
    "    reads = ['you say hel',\n",
    "                ' say hello wo',\n",
    "                        'lo world, i be',\n",
    "                            'ld, i bellow go t',\n",
    "                                        'ow go to hell']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 3\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_3_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "    reads = ['she_sells_s',\n",
    "                'lls_sea_shel',\n",
    "                        'ea_shells_o',\n",
    "                        'shells_on_the_s',\n",
    "                                    'he_sea_s',\n",
    "                                        'ea_shore']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_3_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "    reads = ['she_sells_s',\n",
    "                'lls_sea_shel',\n",
    "                        'ea_shells_o',\n",
    "                        'shells_on_the_s',\n",
    "                                    'he_sea_s',\n",
    "                                        'ea_shore']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 generated sequences\n",
    "# sequitur\n",
    "n = 5\n",
    "m = 100\n",
    "for seed in range(n):  \n",
    "    with open('data/output/generated_sequence_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "        f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "        seq = generate_genome_sequence(10000,seed=seed)\n",
    "        reads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "        reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "        rows = list(range(len(reads)))\n",
    "        cols = list(range(len(reads)))\n",
    "        for _ in range(m):\n",
    "            suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "            B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "            start = time.time()\n",
    "            B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "            if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "            if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "            t2 += time.time() - start\n",
    "            seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "for seed in range(n): \n",
    "    with open('data/output/generated_sequence_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "        f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "        seq = generate_genome_sequence(10000,seed=seed)\n",
    "        reads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for _ in range(m):\n",
    "            for k in range(a,b):\n",
    "                G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "                outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "            for k in outputs_seq:\n",
    "                f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving stored sparse bipartite adjacency matrix...\n",
      "Commencing sequence construction...\n",
      "Sequence reconstruction successful.\n"
     ]
    }
   ],
   "source": [
    "# real genomic sequence, generated reads\n",
    "# sequitur\n",
    "n = 5\n",
    "m = 10\n",
    "for seed in range(n):\n",
    "    with open('data/output/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "        f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "        for record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "        reads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "        reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "        rows = list(range(len(reads)))\n",
    "        cols = list(range(len(reads)))\n",
    "        for _ in range(m):\n",
    "            suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "            B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "            start = time.time()\n",
    "            B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "            if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "            if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "            t2 += time.time() - start\n",
    "            if not os.path.exists('data/input/matrices/seed_'+str(seed)+'.npz'):\n",
    "                save_npz('data/input/matrices/seed_'+str(seed)+'.npz', B)\n",
    "            seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "for seed in range(n):\n",
    "    with open('data/output/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "        f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "        for record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "        reads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for _ in range(m):\n",
    "            for k in range(a,b):\n",
    "                G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "                outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "            for k in outputs_seq:\n",
    "                f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads = []\n",
    "with gzip.open('data/input/JRUI03/JRUI03.1.fsa_nt.gz', 'rt') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        reads += list(read for read in record.seq.split('N') if len(read))\n",
    "with gzip.open('data/input/JRUI03/JRUI03.2.fsa_nt.gz', 'rt') as handle:\n",
    "    for record in SeqIO.parse(handle, 'fasta'):\n",
    "        reads += list(read for read in record.seq.split('N') if len(read))\n",
    "\n",
    "min_suf_len = min(len(read) for read in reads)\n",
    "reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "rows = list(range(len(reads)))\n",
    "cols = list(range(len(reads)))\n",
    "B = create_bipartite_adjacency_matrix(reads,min_suf_len=min_suf_len)\n",
    "B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "save_npz('data/input/matrices/JRUI03.npz', B)\n",
    "seq_ = find_lower_diagonal_path(B,reads_map,cols,rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38731f125b301d8f0df7c54051f2a9a4c898c9398d16ef376d9fb7d661d33405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
