{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "from typing import Union\n",
    "from scipy.sparse import coo_matrix, vstack, hstack, save_npz, load_npz # pip install scipy\n",
    "from jellyfish import damerau_levenshtein_distance # pip install jellyfish\n",
    "import numpy as np # pip install numpy\n",
    "from math import ceil, fmod\n",
    "import networkx as nx # pip install networkx\n",
    "import pylcs # pip install pylcs\n",
    "from Bio import SeqIO # pip install Bio\n",
    "import gzip\n",
    "from fastDamerauLevenshtein import damerauLevenshtein as damerau_levenshtein_distance2 # pip install fastDamerauLevenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are utility functions for generating random sequences and for generating reads of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reads(sequence,min_subseq_len,max_subseq_len,min_overlap,max_overlap,min_coverage=None,circularise=False,seed=None,shuffle=True):\n",
    "\t\"\"\"\n",
    "\tDESCRIPTION \n",
    "\t\tUtility function that chops a sequence into several reads with bounded random lengths that \n",
    "\t\thave a bounded random overlap\n",
    "\tINPUT\n",
    "\t\tsequence       | a sequence of characters that will be divided into overlapping subsequences\n",
    "\t\tmin_subseq_len | the shortest length a subsequence can have\n",
    "\t\tmax_subseq_len | the longest length a subsequence can have\n",
    "\t\tmin_overlap    | the shortest overlap two subsequences can share\n",
    "\t\tmax_overlap    | the longest overlap two subsequences can share\n",
    "\t\tcircularize    | boolean indicating whether to add a random amount of the end of the sequence\n",
    "\t\t\t\t\t| to the beginning and vice versa\n",
    "\t\tseed           | random seed for the random function for reproducibility\n",
    "\tOUTPUT\n",
    "\t\tA list of overlapping reads of random bounded size which share a bounded random amount of\n",
    "\t\toverlap\n",
    "\t\"\"\"\n",
    "\timport random\n",
    "\n",
    "\trandom.seed(seed)\n",
    "\tif circularise: sequence = sequence[-random.randint(min_overlap,max_overlap):] + sequence + sequence[:random.randint(min_overlap,max_overlap)]\n",
    "\treads = []\n",
    "\twhile 1: \n",
    "\t\tstart = 0\n",
    "\t\tend = random.randint(min_subseq_len,max_subseq_len)\n",
    "\t\treads += [sequence[start:end]]\n",
    "\t\twhile end < len(sequence):\n",
    "\t\t\tstart = random.randint(end-max_overlap,end-min_overlap)\n",
    "\t\t\tif (len(sequence) - start)/max_subseq_len < 2:\n",
    "\t\t\t\tif (len(sequence) - start)/max_subseq_len < 1:\n",
    "\t\t\t\t\tend = len(sequence)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ta = 0\n",
    "\t\t\t\t\twhile (len(sequence) - start)/(min_subseq_len+a) > 2: a+=1\n",
    "\t\t\t\t\tend = random.randint(start+min_subseq_len+a,start+max_subseq_len) \n",
    "\t\t\telse: end = random.randint(start+min_subseq_len,start+max_subseq_len) \n",
    "\t\t\treads += [sequence[start:end]]\n",
    "\t\tif min_coverage is None or len(set(reads))*(sum(len(read) for read in set(reads))/len(set(reads)))/len(sequence) >= min_coverage:\n",
    "\t\t\tif not shuffle: return reads\n",
    "\t\t\treads_ = reads[:]\n",
    "\t\t\trandom.shuffle(reads_)\n",
    "\t\t\treturn reads_, list(reads_.index(read) for read in reads)\n",
    "\n",
    "\n",
    "def generate_genome_sequence(n,palindrome=False,seed=None):\n",
    "\t\"\"\"\n",
    "\tDESCRIPTION \n",
    "\t\tUtility function that creates a random sequence containing only the letters A, T, G, and C\n",
    "\tINPUT\n",
    "\t\tn          | the length of the sequence\n",
    "\t\tpalindrome | a boolean indicating whether the sequence must be a palidrome or not\n",
    "\t\tseed       | random seed for the random function for reproducibility\n",
    "\tOUTPUT\n",
    "\t\tA random sequence of length n\n",
    "\t\"\"\"\n",
    "\timport random\n",
    "\t\n",
    "\trandom.seed(seed)\n",
    "\tnucleotides = {1:'A',2:'C',3:'G',4:'T'}\n",
    "\tseq = ''\n",
    "\tif palindrome: n = ceil(n/2)\n",
    "\tfor _ in range(n):\n",
    "\t\tseq += nucleotides[random.randint(1,4)]\n",
    "\tif palindrome: seq += ''.join(reversed(seq[:int(n-fmod(n,2))]))\n",
    "\treturn seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Bruijn Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation for a De Bruijn Graph assembler. In reality, many statistical, optimisation, and computation techniques are implemented to improve the efficiency and quality of input. Here we use a basic technique to compare to our basic technique. The idea being that with the addition of similar developments, the novel technique presented here might be able to at least match or perhaps surpass this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_overlap(reads: list) -> int:\n",
    "\toverlaps = []\n",
    "\tfor read in reads:\n",
    "\t\toverlaps += pylcs.lcs2_of_list(read, list(set(reads).symmetric_difference([read])))\n",
    "\treturn min(overlaps),max(overlaps)\n",
    "\n",
    "def create_de_bruijn_graph(k: int, sequences: list,do_time: bool = False) -> Union[nx.Graph,tuple]:\n",
    "\t\"\"\"\n",
    "\tCreate a de Bruijn graph from a set of DNA sequences.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- k (int): k-mer size\n",
    "\t- sequences (list): List of DNA sequences\n",
    "\t- do_time (bool): should the function be time of not (default False)\n",
    "\t\n",
    "\tReturns:\n",
    "\t- Union[nx.Graph,(nx.Graph,float)]: De Bruijn graph or a tuple of the De Bruijn graph and the time\n",
    "\t\t\t\t\t\t\t\t\t\tto create it.\n",
    "\t\"\"\"\n",
    "\tif do_time: start_time = time.time()\n",
    "\tgraph = nx.DiGraph()\n",
    "\n",
    "\tfor sequence in sequences:\n",
    "\t\tfor i in range(len(sequence) - k + 1):\n",
    "\t\t\tkmer = sequence[i:i+k]\n",
    "\t\t\tprefix = kmer[:-1]\n",
    "\t\t\tsuffix = kmer[1:]\n",
    "\t\t\t\n",
    "\t\t\tif not graph.has_edge(prefix, suffix):\n",
    "\t\t\t\tgraph.add_edge(prefix, suffix, weight=1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgraph[prefix][suffix]['weight'] += 1\n",
    "\n",
    "\tif do_time: return graph, time.time() - start_time\n",
    "\treturn graph\n",
    "\n",
    "def eulerian_path(graph: nx.DiGraph,do_time: bool = False) -> Union[str,tuple]:\n",
    "\t\"\"\"\n",
    "\tFind an Eulerian path in the given graph.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- graph (nx.Graph): De Bruijn graph\n",
    "\t- do_time (bool): should the function be time of not (default False)\n",
    "\t\n",
    "\tReturns:\n",
    "\t- Union[str,(str,float)]: string describing the eulerian path or a tuple of this \n",
    "\t\t\t\t\t\t\t  string and the execution time.\n",
    "\t\"\"\"\n",
    "\tif do_time: start_time = time.time()\n",
    "\tpath = []\n",
    "\t\n",
    "\tif nx.has_eulerian_path(graph):\n",
    "\t# try: \n",
    "\t#     graph_ = graph.to_undirected()\n",
    "\t#     graph_ = nx.eulerize(graph_)\n",
    "\t\tfor node in nx.eulerian_path(graph):\n",
    "\t\t\tif len(path): path.append(node[0][-1])\n",
    "\t\t\telse: path.append(node[0])\n",
    "\t\t\n",
    "\t\tif len(node[1]): path.append(node[1][-1])\n",
    "\t\telse: path.append(node[1])\n",
    "\t\t\n",
    "\t\tif do_time: return ''.join(path), time.time() - start_time\n",
    "\t\treturn ''.join(path)\n",
    "\t# except:\n",
    "\t#     if time: return '', time.time() - start_time\n",
    "\telse: \n",
    "\t\tif time: return '', time.time() - start_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods necessary for implementing the Sequitur assembly technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_damerau_levenshtein_distance(read: str,overlap: str) -> float:\n",
    "\t\"\"\"\n",
    "\tFind the Damerau-Levenshtein edit distance of two strings normalised to the length\n",
    "\tof the shorter string. This normalisation is because we want to path prefixes to\n",
    "\tsuffixes and this means that in general we will be comparing a full string to a\n",
    "\tportion of another string.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- read (str): string for comparison, usually the longer string \n",
    "\t- overlap (str): string for comparison, usually the shorter string\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float: the normalised Demarau-Levenshtein edit distance of the input strings\n",
    "\t\"\"\"\n",
    "\treturn damerau_levenshtein_distance(read.__str__()[:min(len(overlap),len(read))],overlap.__str__()[:min(len(overlap),len(read))])/min(len(overlap),len(read))\n",
    "\n",
    "def build_suffix_array(reads: list, min_suf_len: int = 3,do_time: bool = False) -> tuple:\n",
    "\tif do_time: start = time.time()\n",
    "\tsuf_arr = []\n",
    "\tfor read in reads:\n",
    "\t\tread += '$' + str(reads.index(read))\n",
    "\t\tfor i in range(len(read)-min_suf_len-1):\n",
    "\t\t\t# if len(read[i:]) < min_suf_len + 2: continue \n",
    "\t\t\tsuf_arr += [read[i:]]\n",
    "\tsuf_arr.sort()\n",
    "\tsuf_arr_ind = []\n",
    "\tfor s in range(len(suf_arr)):\n",
    "\t\tsuf_arr_ind += [int(suf_arr[s].split('$')[-1].__str__())]\n",
    "\t\tsuf_arr[s] = suf_arr[s][:suf_arr[s].find('$')+1]\n",
    "\tif do_time: return suf_arr, suf_arr_ind, time.time() - start\n",
    "\treturn suf_arr,suf_arr_ind\n",
    "\n",
    "def create_bipartite_adjacency_matrix(reads: list, suf_arr: list = None, suf_arr_ind: list = None, do_time: bool = False,max_diff: float = 0.25, min_suf_len: int = 3) -> dict:\n",
    "\tif do_time: start = time.time()\n",
    "\tif suf_arr is None or suf_arr_ind is None: suf_arr,suf_arr_ind = build_suffix_array(reads,min_suf_len=min_suf_len)\n",
    "\treads_map = dict(zip(reads,list(range(len(reads)))))\n",
    "\tB = {}\n",
    "\tfor read in reads:\n",
    "\t\tfor j in range(min_suf_len + 1):\n",
    "\t\t\ti = suf_arr.index(read[j:]+'$') - 1\n",
    "\t\t\twhile normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) <= 0.5:\n",
    "\t\t\t\tif not reads[suf_arr_ind[i]] == read and \\\n",
    "\t\t\t\t   normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) < max_diff and \\\n",
    "\t\t\t\t   read.startswith(suf_arr[i][:-1]):\n",
    "\t\t\t\t\tif (reads_map[reads[suf_arr_ind[i]]],reads_map[read]) not in B: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = len(suf_arr[i][:-1])\n",
    "\t\t\t\t\telse: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = max(len(suf_arr[i][:-1]),B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])])\n",
    "\t\t\t\ti -= 1\n",
    "\tif do_time: return B, time.time() - start\n",
    "\treturn B\n",
    "\n",
    "def move_col(B: coo_matrix, cols: dict) -> None:\n",
    "\tfor c in range(len(B.col)):\n",
    "\t\tB.col[c] = cols[B.col[c]]\n",
    "\t\t\t\n",
    "def move_row(B: coo_matrix,rows: dict) -> None:\n",
    "\tfor r in range(len(B.row)):\n",
    "\t\tB.row[r] = rows[B.row[r]]\n",
    "\n",
    "def find_lower_diagonal_path(B: coo_matrix,reads_map: dict,cols: list,rows: list,do_time: bool = False) -> tuple:\n",
    "\tif do_time: start = time.time()\n",
    "\targpen = lambda l: np.argpartition(l,-2)[-2]\n",
    "\n",
    "\tnew_cols = cols[:]\n",
    "\tif B.sum(axis=0).min() == 0: new_cols = list(c for c in new_cols if c not in [new_cols[B.sum(axis=0).argmin()]]) + [new_cols[B.sum(axis=0).argmin()]]\n",
    "\tif B.sum(axis=1).min() == 0: \n",
    "\t\tif B.sum(axis=1).argmin() == B.sum(axis=0).argmin():\n",
    "\t\t\tnew_cols = [new_cols[-1]] + list(c for c in new_cols[:-1] if c not in [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]) + [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]\n",
    "\t\telse: new_cols = [rows[B.sum(axis=1).argmin()]] + list(c for c in new_cols if c not in [rows[B.sum(axis=1).argmin()]])\n",
    "\n",
    "\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\tmove_col(B,cols_map)\n",
    "\tcols = new_cols\n",
    "\n",
    "\tnew_rows = cols[:]\n",
    "\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\tmove_row(B,rows_map)\n",
    "\trows = new_rows\n",
    "\n",
    "\ti,j,k = len(rows), len(cols) - 1, B.sum(axis=1).argmin() if B.sum(axis=1).min() == 0 else None\n",
    "\n",
    "\twhile j > (k if B.sum(axis=1).min() == 0 else 0):\n",
    "\t\tif k is not None and B.getrow(rows.index(cols[j])).argmax() == k: \n",
    "\t\t\tcols_,c_ = [], 0\n",
    "\n",
    "\t\t\twhile j + c_ + 1 < len(rows):\n",
    "\t\t\t\tc_ += 1\n",
    "\t\t\t\tif len(B.getrow(j+c_).nonzero()[1]) > 1:\n",
    "\t\t\t\t\tcols_ = np.argpartition(B.getrow(j+c_).toarray().flatten(),-2)[::-1][:2]\n",
    "\t\t\t\t\tif cols[cols_[1]] in cols[:j] and B.getcol(cols_[1]).argmax() == j+c_: break\n",
    "\t\t\t\n",
    "\t\t\tif j + c_ + 1 == len(cols): new_cols = cols[:k+1] + cols[j:] + cols[k+1:j]\n",
    "\t\t\telse: new_cols = cols[:k+1] + cols[j:j+c_] + list(c for c in cols[k+1:j] if c not in [cols[min(cols_)]]) + [cols[min(cols_)]] + cols[j+c_:]\n",
    "\t\t\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\t\t\tmove_col(B,cols_map)\n",
    "\t\t\tcols = new_cols\n",
    "\n",
    "\t\t\tif j + c_ + 1 == len(rows): new_rows = cols[:]\n",
    "\t\t\telse: new_rows = cols[:k+c_+1] + list(r for r in rows[k:j+c_] if r not in cols[:k+c_+1] + cols[j+c_:]) + cols[j+c_:]\n",
    "\t\t\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\t\t\tmove_row(B,rows_map)\n",
    "\t\t\trows = new_rows\n",
    "\n",
    "\t\t\ti,j,k = j + c_ + 1, j + c_, k + c_\n",
    "\t\telse:\n",
    "\t\t\tcmax = B.getrow(rows.index(cols[j])).argmax()\n",
    "\t\t\tif len(B.getrow(rows.index(cols[j])).nonzero()[1]) > 1:\n",
    "\t\t\t\tcpen = argpen(B.getrow(rows.index(cols[j])).toarray().flatten()) \n",
    "\t\t\t\tif cmax > j: \n",
    "\t\t\t\t\tif len(B.getrow(cmax+1).nonzero()[1]) > 1 and \\\n",
    "\t\t\t\t\tB.getrow(cmax+1).getcol(argpen(B.getrow(cmax+1).toarray().flatten())).data[0] >=  B.getrow(rows.index(cols[j])).getcol(cpen).data[0]: \n",
    "\t\t\t\t\t\tcrange = [argpen(B.getrow(cmax).toarray().flatten()),cmax]\n",
    "\t\t\t\t\telse: crange = [cpen]\n",
    "\t\t\t\telse: crange = [cmax]\n",
    "\t\t\telse: crange = [cmax]\n",
    "\t\t\twhile crange[0] > j:\n",
    "\t\t\t\tif len(B.getrow(crange[0]).nonzero()[1]) > 1:\n",
    "\t\t\t\t\tcrange = [argpen(B.getrow(crange[0]).toarray().flatten())] + crange\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcrange = [B.getrow(crange[0]).argmax()] + crange\n",
    "\t\t\t\tif crange[0] == j: crange = [B.getrow(crange[1]).argmax()] + crange[1:]\n",
    "\n",
    "\t\t\tnew_cols = list(c for c in cols[:j] if c not in list(cols[cr] for cr in crange)) + list(cols[cr] for cr in crange) + list(c for c in cols[j:] if c not in list(cols[cr] for cr in crange))\n",
    "\t\t\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\t\t\tmove_col(B,cols_map)\n",
    "\t\t\tcols = new_cols\n",
    "\n",
    "\t\t\tnew_rows = list(r for r in rows[:i] if r not in cols[j:]) + cols[j:]\n",
    "\t\t\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\t\t\tmove_row(B,rows_map)\n",
    "\t\t\trows = new_rows\n",
    "\t\tj -= 1\n",
    "\t\ti -= 1\n",
    "\n",
    "\tseq = ''\n",
    "\tfor s,d in zip(list(reads_map[k] for k in rows)[:-1],B.diagonal(-1)):\n",
    "\t\tseq += s[:-d]\n",
    "\tseq += list(reads_map[k] for k in rows)[-1]\n",
    "\tif do_time: return seq, time.time() - start\n",
    "\treturn seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "READS = [['betty_bought_butter_th',\n",
    "\t\t\t\t 'tter_the_butter_was_',\n",
    "\t\t\t\t\t 'he_butter_was_bitter_',\n",
    "\t\t\t\t\t\t\t'as_bitter_betty_bought',\n",
    "\t\t\t\t\t\t\t\t\t'tty_bought_better_butter_t',\n",
    "\t\t\t\t\t\t\t\t\t\t'ught_better_butter_to',\n",
    "\t\t\t\t\t\t\t\t\t\t\t    'r_butter_to_make_the_',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ke_the_bitter_butter_better'],\n",
    "\t   ['you say hel',\n",
    "\t\t   'say hello wo',\n",
    "\t\t\t    'lo world, i be',\n",
    "\t\t\t\t    'ld, i bellow go t',\n",
    "\t\t\t\t\t\t  'ow go to hell'],\n",
    "\t   ['she_sells_s',\n",
    "\t\t    'lls_sea_shel',\n",
    "\t\t\t   'ea_shells_o',\n",
    "\t\t\t\t'shells_on_the_s',\n",
    "\t\t\t\t\t     'he_sea_s',\n",
    "\t\t\t\t\t\t   'ea_shore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "reads = ['betty_bought_butter_th',\n",
    "\t\t\t\t\t\t'tter_the_butter_was_',\n",
    "\t\t\t\t\t\t\t'he_butter_was_bitter_',\n",
    "\t\t\t\t\t\t\t\t\t'as_bitter_betty_bought',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'tty_bought_better_butter_t',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'ught_better_butter_to',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'r_butter_to_make_the_',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ke_the_bitter_butter_better']\n",
    "random.seed(seed)\n",
    "random.shuffle(reads)\n",
    "reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "rows = list(range(len(reads)))\n",
    "cols = list(range(len(reads)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = load_npz('data/input/matrices/seed_0.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import maximum_bipartite_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse.csgraph import min_weight_full_bipartite_matching\n",
    "# error: no full matching exists\n",
    "# because: no edges on the start and end nodes. \n",
    "# fix: could probably remove that row and col but :shrug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int32(986)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_bipartite_matching(B.tocsr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq('CAATCTCATATGCCGAGATCAAGGTTCAAACAAAAACAAGAACAGAAATAGATC...GAT')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads_map[986]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "for record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "reads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "random.seed(seed)\n",
    "random.shuffle(reads)\n",
    "reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "rows = list(range(len(reads)))\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(90)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.tocsr()[maximum_bipartite_matching(B.tocsr())].diagonal().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[50]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.getrow(1291).getcol(683).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Seq('GCTGAAAAGACAATTTGGAATCTCCTCCGTAGTAACCTGTTGCAGAAGAT'),\n",
       " Seq('CTCCTGCCTACAGGGCATATCCCACAACAGGTATTAGCTTTTGACAATGA'),\n",
       " Seq('CACCTTGATCTGATGTAATTCTGCGGATTGCATTAGCAGATTTTCTGGCT'),\n",
       " Seq('GATACATTTGATTCTGCTTTTCCCAATAATATCAAGCGGGTATAACTCTA'))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reads_map[1291][:50],reads_map[683][:50],reads_map[1291][-50:],reads_map[683][-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq('CCCCTTGAGTCTCTAACATCAGAATCCGAACAAAAGACCTACAGGTCGGCCACA...GAA')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int32(-1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m seq_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreads_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmaximum_bipartite_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtocsr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],B\u001b[38;5;241m.\u001b[39mtocsr()[maximum_bipartite_matching(B\u001b[38;5;241m.\u001b[39mtocsr())]\u001b[38;5;241m.\u001b[39mdiagonal()):\n\u001b[1;32m      3\u001b[0m \tseq_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m s[:\u001b[38;5;241m-\u001b[39md]\n\u001b[1;32m      4\u001b[0m seq_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reads_map[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m rows)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[91], line 2\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m seq_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s,d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mreads_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m maximum_bipartite_matching(B\u001b[38;5;241m.\u001b[39mtocsr()))[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],B\u001b[38;5;241m.\u001b[39mtocsr()[maximum_bipartite_matching(B\u001b[38;5;241m.\u001b[39mtocsr())]\u001b[38;5;241m.\u001b[39mdiagonal()):\n\u001b[1;32m      3\u001b[0m \tseq_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m s[:\u001b[38;5;241m-\u001b[39md]\n\u001b[1;32m      4\u001b[0m seq_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(reads_map[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m rows)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: np.int32(-1)"
     ]
    }
   ],
   "source": [
    "seq_ = ''\n",
    "for s,d in zip(list(reads_map[k] for k in maximum_bipartite_matching(B.tocsr()))[:-1],B.tocsr()[maximum_bipartite_matching(B.tocsr())].diagonal()):\n",
    "\tseq_ += s[:-d]\n",
    "# seq_ += list(reads_map[k] for k in rows)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq('CCCCTTGAGTCTCTAACATCAGAATCCGAACAAAAGACCTACAGGTCGGCCACA...GAA')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq('TCGCTCCTTGCTTGGGTCTAGGAGAAGAGAGAAGAACGAGGGTGAAACGCACTT...GCC')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 1\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_1_sequitur__.csv','a') as f:\n",
    "\tf.write('natural_language_sequence,edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\tseq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "\treads = ['betty_bought_butter_th',\n",
    "\t\t\t\t\t\t\t'tter_the_butter_was_',\n",
    "\t\t\t\t\t\t\t\t'he_butter_was_bitter_',\n",
    "\t\t\t\t\t\t\t\t\t\t'as_bitter_betty_bought',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'tty_bought_better_butter_t',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ught_better_butter_to',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'r_butter_to_make_the_',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ke_the_bitter_butter_better']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\trows = list(range(len(reads)))\n",
    "\tcols = list(range(len(reads)))\n",
    "\tfor _ in range(n):\n",
    "\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\tstart = time.time()\n",
    "\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\tt2 += time.time() - start\n",
    "\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\tf.write('1,{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_1_euler__.csv','a') as f:\n",
    "\tf.write('natural language_sequence,k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\tseq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "\treads = ['betty_bought_butter_th',\n",
    "\t\t\t\t\t\t\t'tter_the_butter_was_',\n",
    "\t\t\t\t\t\t\t\t'he_butter_was_bitter_',\n",
    "\t\t\t\t\t\t\t\t\t\t'as_bitter_betty_bought',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'tty_bought_better_butter_t',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ught_better_butter_to',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'r_butter_to_make_the_',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ke_the_bitter_butter_better']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\tfor _ in range(n):\n",
    "\t\toutputs_seq = {}\n",
    "\t\toutputs_dbg_time = {}\n",
    "\t\toutputs_euler_time = {}\n",
    "\t\ta,b = find_longest_overlap(reads)\n",
    "\t\tfor k in range(a,b):\n",
    "\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\tfor k in outputs_seq:\n",
    "\t\t\tf.write('1,{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m---> 54\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meulerize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39meulerian_path(graph):\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:3\u001b[0m, in \u001b[0;36margmap_eulerize_1\u001b[1;34m(G, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\networkx\\utils\\decorators.py:90\u001b[0m, in \u001b[0;36mnot_implemented_for.<locals>._not_implemented_for\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_multigraph()) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     88\u001b[0m     dval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_directed()\n\u001b[0;32m     89\u001b[0m ):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXNotImplemented(errmsg)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m: not implemented for directed type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a,b):\n\u001b[0;32m     47\u001b[0m     G,outputs_dbg_time[k] \u001b[38;5;241m=\u001b[39m create_de_bruijn_graph(k,reads,do_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 48\u001b[0m     outputs_seq[k],outputs_euler_time[k] \u001b[38;5;241m=\u001b[39m \u001b[43meulerian_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdo_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs_seq:\n\u001b[0;32m     50\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k,damerau_levenshtein_distance(outputs_seq[k],seq),\u001b[38;5;28mlen\u001b[39m(seq),\u001b[38;5;28mlen\u001b[39m(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# natural language sequence 2\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_2_sequitur.csv','a') as f:\n",
    "\tf.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\tseq = 'you say hello world, i bellow go to hell'\n",
    "\treads = ['you say hel',\n",
    "\t\t\t\t' say hello wo',\n",
    "\t\t\t\t\t\t'lo world, i be',\n",
    "\t\t\t\t\t\t\t'ld, i bellow go t',\n",
    "\t\t\t\t\t\t\t\t\t\t'ow go to hell']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\trows = list(range(len(reads)))\n",
    "\tcols = list(range(len(reads)))\n",
    "\tfor _ in range(n):\n",
    "\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\tstart = time.time()\n",
    "\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\tt2 += time.time() - start\n",
    "\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\tf.write('1,{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_2_euler.csv','a') as f:\n",
    "\tf.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\tseq = 'you say hello world, i bellow go to hell'\n",
    "\treads = ['you say hel',\n",
    "\t\t\t\t' say hello wo',\n",
    "\t\t\t\t\t\t'lo world, i be',\n",
    "\t\t\t\t\t\t\t'ld, i bellow go t',\n",
    "\t\t\t\t\t\t\t\t\t\t'ow go to hell']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\tfor _ in range(n):\n",
    "\t\toutputs_seq = {}\n",
    "\t\toutputs_dbg_time = {}\n",
    "\t\toutputs_euler_time = {}\n",
    "\t\ta,b = find_longest_overlap(reads)\n",
    "\t\tfor k in range(a,b):\n",
    "\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\tfor k in outputs_seq:\n",
    "\t\t\tf.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 3\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_3_sequitur.csv','a') as f:\n",
    "\tf.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\tseq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "\treads = ['she_sells_s',\n",
    "\t\t\t\t'lls_sea_shel',\n",
    "\t\t\t\t\t\t'ea_shells_o',\n",
    "\t\t\t\t\t\t'shells_on_the_s',\n",
    "\t\t\t\t\t\t\t\t\t'he_sea_s',\n",
    "\t\t\t\t\t\t\t\t\t\t'ea_shore']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\trows = list(range(len(reads)))\n",
    "\tcols = list(range(len(reads)))\n",
    "\tfor _ in range(n):\n",
    "\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\tstart = time.time()\n",
    "\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\tt2 += time.time() - start\n",
    "\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\tf.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_3_euler.csv','a') as f:\n",
    "\tf.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\tseq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "\treads = ['she_sells_s',\n",
    "\t\t\t\t'lls_sea_shel',\n",
    "\t\t\t\t\t\t'ea_shells_o',\n",
    "\t\t\t\t\t\t'shells_on_the_s',\n",
    "\t\t\t\t\t\t\t\t\t'he_sea_s',\n",
    "\t\t\t\t\t\t\t\t\t\t'ea_shore']\n",
    "\trandom.seed(seed)\n",
    "\trandom.shuffle(reads)\n",
    "\tfor _ in range(n):\n",
    "\t\toutputs_seq = {}\n",
    "\t\toutputs_dbg_time = {}\n",
    "\t\toutputs_euler_time = {}\n",
    "\t\ta,b = find_longest_overlap(reads)\n",
    "\t\tfor k in range(a,b):\n",
    "\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\tfor k in outputs_seq:\n",
    "\t\t\tf.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 generated sequences\n",
    "# sequitur\n",
    "n = 5\n",
    "m = 100\n",
    "for seed in range(n):  \n",
    "\twith open('data/output/generated_sequence_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "\t\tf.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\t\tseq = generate_genome_sequence(10000,seed=seed)\n",
    "\t\treads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "\t\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\t\trows = list(range(len(reads)))\n",
    "\t\tcols = list(range(len(reads)))\n",
    "\t\tfor _ in range(m):\n",
    "\t\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\t\tstart = time.time()\n",
    "\t\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\t\tt2 += time.time() - start\n",
    "\t\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\t\tf.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "for seed in range(n): \n",
    "\twith open('data/output/generated_sequence_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "\t\tf.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\t\tseq = generate_genome_sequence(10000,seed=seed)\n",
    "\t\treads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "\t\toutputs_seq = {}\n",
    "\t\toutputs_dbg_time = {}\n",
    "\t\toutputs_euler_time = {}\n",
    "\t\ta,b = find_longest_overlap(reads)\n",
    "\t\tfor _ in range(m):\n",
    "\t\t\tfor k in range(a,b):\n",
    "\t\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "\t\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\t\tfor k in outputs_seq:\n",
    "\t\t\t\tf.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real genomic sequence, generated reads\n",
    "# sequitur\n",
    "n = 1\n",
    "m = 1\n",
    "seed = 37\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/sequitur/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "\tf.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\tfor record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "\treads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\trows = list(range(len(reads)))\n",
    "\tcols = list(range(len(reads)))\n",
    "\tfor _ in range(m):\n",
    "\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\tstart = time.time()\n",
    "\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\tt2 += time.time() - start\n",
    "\t\tif not os.path.exists('data/input/matrices/seed_'+str(seed)+'.npz'):\n",
    "\t\t\tsave_npz('data/input/matrices/seed_'+str(seed)+'.npz', B)\n",
    "\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\tf.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance2(str(seq_),str(seq),similarity=False),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/euler/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "\tf.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\tfor record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "\treads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "\toutputs_seq = {}\n",
    "\toutputs_dbg_time = {}\n",
    "\toutputs_euler_time = {}\n",
    "\ta,b = find_longest_overlap(list(str(read) for read in reads))\n",
    "\tfor _ in range(m):\n",
    "\t\tfor k in range(a,b):\n",
    "\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,list(str(read) for read in reads),do_time=True)\n",
    "\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\tfor k in outputs_seq:\n",
    "\t\t\tf.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance2(outputs_seq[k],seq,similarity=False),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38731f125b301d8f0df7c54051f2a9a4c898c9398d16ef376d9fb7d661d33405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
