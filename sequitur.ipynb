{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastDamerauLevenshtein\n",
      "  Using cached fastDamerauLevenshtein-1.0.7.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.12.0-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "     ---------------------------------------- 46.2/46.2 MB 4.5 MB/s eta 0:00:00\n",
      "Collecting jellyfish\n",
      "  Downloading jellyfish-1.0.3-cp39-none-win_amd64.whl (210 kB)\n",
      "     -------------------------------------- 211.0/211.0 KB 3.2 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.26.4-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 5.0 MB/s eta 0:00:00\n",
      "Collecting pylcs\n",
      "  Downloading pylcs-0.1.1-cp39-cp39-win_amd64.whl (78 kB)\n",
      "     ---------------------------------------- 79.0/79.0 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting Bio\n",
      "  Using cached bio-1.6.2-py3-none-any.whl (278 kB)\n",
      "Collecting pybind11>=2.2\n",
      "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "Collecting pooch\n",
      "  Using cached pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.3/78.3 KB 4.5 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.1-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 11.6/11.6 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting biopython>=1.80\n",
      "  Downloading biopython-1.83-cp39-cp39-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting gprofiler-official\n",
      "  Using cached gprofiler_official-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting mygene\n",
      "  Using cached mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
      "Collecting biothings-client>=0.2.6\n",
      "  Using cached biothings_client-0.3.1-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\a0068449\\appdata\\roaming\\python\\python39\\site-packages (from pandas->Bio) (2.9.0.post0)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "     -------------------------------------- 345.4/345.4 KB 4.3 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "     -------------------------------------- 505.5/505.5 KB 5.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\a0068449\\appdata\\roaming\\python\\python39\\site-packages (from pooch->Bio) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\a0068449\\appdata\\roaming\\python\\python39\\site-packages (from pooch->Bio) (24.0)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "     -------------------------------------- 100.4/100.4 KB 5.6 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 61.6/61.6 KB 3.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     ------------------------------------- 163.8/163.8 KB 10.2 MB/s eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     -------------------------------------- 121.1/121.1 KB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\a0068449\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->Bio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\a0068449\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->Bio) (1.16.0)\n",
      "Using legacy 'setup.py install' for fastDamerauLevenshtein, since package 'wheel' is not installed.\n",
      "Installing collected packages: pytz, fastDamerauLevenshtein, urllib3, tzdata, tqdm, pybind11, numpy, networkx, jellyfish, idna, charset-normalizer, certifi, scipy, requests, pylcs, pandas, biopython, pooch, gprofiler-official, biothings-client, mygene, Bio\n",
      "  Running setup.py install for fastDamerauLevenshtein: started\n",
      "  Running setup.py install for fastDamerauLevenshtein: finished with status 'done'\n",
      "Successfully installed Bio-1.6.2 biopython-1.83 biothings-client-0.3.1 certifi-2024.2.2 charset-normalizer-3.3.2 fastDamerauLevenshtein-1.0.7 gprofiler-official-1.0.0 idna-3.6 jellyfish-1.0.3 mygene-3.2.2 networkx-3.2.1 numpy-1.26.4 pandas-2.2.1 pooch-1.8.1 pybind11-2.11.1 pylcs-0.1.1 pytz-2024.1 requests-2.31.0 scipy-1.12.0 tqdm-4.66.2 tzdata-2024.1 urllib3-2.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\A0068449\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "! pip install fastDamerauLevenshtein scipy jellyfish numpy networkx pylcs Bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "from typing import Union\n",
    "from scipy.sparse import coo_matrix, vstack, hstack, save_npz, load_npz # pip install scipy\n",
    "from jellyfish import damerau_levenshtein_distance # pip install jellyfish\n",
    "import numpy as np # pip install numpy\n",
    "from math import ceil, fmod\n",
    "import networkx as nx # pip install networkx\n",
    "import pylcs # pip install pylcs\n",
    "from Bio import SeqIO # pip install Bio\n",
    "import gzip\n",
    "from fastDamerauLevenshtein import damerauLevenshtein as damerau_levenshtein_distance2 # pip install fastDamerauLevenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are utility functions for generating random sequences and for generating reads of sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_reads(sequence,min_subseq_len,max_subseq_len,min_overlap,max_overlap,min_coverage=None,circularise=False,seed=None,shuffle=True):\n",
    "    \"\"\"\n",
    "    DESCRIPTION \n",
    "        Utility function that chops a sequence into several reads with bounded random lengths that \n",
    "        have a bounded random overlap\n",
    "    INPUT\n",
    "        sequence       | a sequence of characters that will be divided into overlapping subsequences\n",
    "        min_subseq_len | the shortest length a subsequence can have\n",
    "        max_subseq_len | the longest length a subsequence can have\n",
    "        min_overlap    | the shortest overlap two subsequences can share\n",
    "        max_overlap    | the longest overlap two subsequences can share\n",
    "        circularize    | boolean indicating whether to add a random amount of the end of the sequence\n",
    "                    | to the beginning and vice versa\n",
    "        seed           | random seed for the random function for reproducibility\n",
    "    OUTPUT\n",
    "        A list of overlapping reads of random bounded size which share a bounded random amount of\n",
    "        overlap\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    if circularise: sequence = sequence[-random.randint(min_overlap,max_overlap):] + sequence + sequence[:random.randint(min_overlap,max_overlap)]\n",
    "    reads = []\n",
    "    while 1: \n",
    "        start = 0\n",
    "        end = random.randint(min_subseq_len,max_subseq_len)\n",
    "        reads += [sequence[start:end]]\n",
    "        while end < len(sequence):\n",
    "            start = random.randint(end-max_overlap,end-min_overlap)\n",
    "            if (len(sequence) - start)/max_subseq_len < 2:\n",
    "                if (len(sequence) - start)/max_subseq_len < 1:\n",
    "                    end = len(sequence)\n",
    "                else:\n",
    "                    a = 0\n",
    "                    while (len(sequence) - start)/(min_subseq_len+a) > 2: a+=1\n",
    "                    end = random.randint(start+min_subseq_len+a,start+max_subseq_len) \n",
    "            else: end = random.randint(start+min_subseq_len,start+max_subseq_len) \n",
    "            reads += [sequence[start:end]]\n",
    "        if min_coverage is None or len(set(reads))*(sum(len(read) for read in set(reads))/len(set(reads)))/len(sequence) >= min_coverage:\n",
    "            if not shuffle: return reads\n",
    "            reads_ = reads[:]\n",
    "            random.shuffle(reads_)\n",
    "            return reads_, list(reads_.index(read) for read in reads)\n",
    "\n",
    "\n",
    "def generate_genome_sequence(n,palindrome=False,seed=None):\n",
    "    \"\"\"\n",
    "    DESCRIPTION \n",
    "        Utility function that creates a random sequence containing only the letters A, T, G, and C\n",
    "    INPUT\n",
    "        n          | the length of the sequence\n",
    "        palindrome | a boolean indicating whether the sequence must be a palidrome or not\n",
    "        seed       | random seed for the random function for reproducibility\n",
    "    OUTPUT\n",
    "        A random sequence of length n\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    random.seed(seed)\n",
    "    nucleotides = {1:'A',2:'C',3:'G',4:'T'}\n",
    "    seq = ''\n",
    "    if palindrome: n = ceil(n/2)\n",
    "    for _ in range(n):\n",
    "        seq += nucleotides[random.randint(1,4)]\n",
    "    if palindrome: seq += ''.join(reversed(seq[:int(n-fmod(n,2))]))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Bruijn Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation for a De Bruijn Graph assembler. In reality, many statistical, optimisation, and computation techniques are implemented to improve the efficiency and quality of input. Here we use a basic technique to compare to our basic technique. The idea being that with the addition of similar developments, the novel technique presented here might be able to at least match or perhaps surpass this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_overlap(reads: list) -> int:\n",
    "    overlaps = []\n",
    "    for read in reads:\n",
    "        overlaps += pylcs.lcs2_of_list(read, list(set(reads).symmetric_difference([read])))\n",
    "    return min(overlaps),max(overlaps)\n",
    "\n",
    "def create_de_bruijn_graph(k: int, sequences: list,do_time: bool = False) -> Union[nx.Graph,tuple]:\n",
    "    \"\"\"\n",
    "    Create a de Bruijn graph from a set of DNA sequences.\n",
    "    \n",
    "    Parameters:\n",
    "    - k (int): k-mer size\n",
    "    - sequences (list): List of DNA sequences\n",
    "    - do_time (bool): should the function be time of not (default False)\n",
    "    \n",
    "    Returns:\n",
    "    - Union[nx.Graph,(nx.Graph,float)]: De Bruijn graph or a tuple of the De Bruijn graph and the time\n",
    "                                        to create it.\n",
    "    \"\"\"\n",
    "    if do_time: start_time = time.time()\n",
    "    graph = nx.DiGraph()\n",
    "\n",
    "    for sequence in sequences:\n",
    "        for i in range(len(sequence) - k + 1):\n",
    "            kmer = sequence[i:i+k]\n",
    "            prefix = kmer[:-1]\n",
    "            suffix = kmer[1:]\n",
    "            \n",
    "            if not graph.has_edge(prefix, suffix):\n",
    "                graph.add_edge(prefix, suffix, weight=1)\n",
    "            else:\n",
    "                graph[prefix][suffix]['weight'] += 1\n",
    "\n",
    "    if do_time: return graph, time.time() - start_time\n",
    "    return graph\n",
    "\n",
    "def eulerian_path(graph: nx.DiGraph,do_time: bool = False) -> Union[str,tuple]:\n",
    "    \"\"\"\n",
    "    Find an Eulerian path in the given graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph (nx.Graph): De Bruijn graph\n",
    "    - do_time (bool): should the function be time of not (default False)\n",
    "    \n",
    "    Returns:\n",
    "    - Union[str,(str,float)]: string describing the eulerian path or a tuple of this \n",
    "                              string and the execution time.\n",
    "    \"\"\"\n",
    "    if do_time: start_time = time.time()\n",
    "    path = []\n",
    "    \n",
    "    if nx.has_eulerian_path(graph):\n",
    "    # try: \n",
    "    #     graph_ = graph.to_undirected()\n",
    "    #     graph_ = nx.eulerize(graph_)\n",
    "        for node in nx.eulerian_path(graph):\n",
    "            if len(path): path.append(node[0][-1])\n",
    "            else: path.append(node[0])\n",
    "        \n",
    "        if len(node[1]): path.append(node[1][-1])\n",
    "        else: path.append(node[1])\n",
    "        \n",
    "        if do_time: return ''.join(path), time.time() - start_time\n",
    "        return ''.join(path)\n",
    "    # except:\n",
    "    #     if time: return '', time.time() - start_time\n",
    "    else: \n",
    "        if time: return '', time.time() - start_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods necessary for implementing the Sequitur assembly technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_damerau_levenshtein_distance(read: str,overlap: str) -> float:\n",
    "    \"\"\"\n",
    "    Find the Damerau-Levenshtein edit distance of two strings normalised to the length\n",
    "    of the shorter string. This normalisation is because we want to path prefixes to\n",
    "    suffixes and this means that in general we will be comparing a full string to a\n",
    "    portion of another string.\n",
    "    \n",
    "    Parameters:\n",
    "    - read (str): string for comparison, usually the longer string \n",
    "    - overlap (str): string for comparison, usually the shorter string\n",
    "    \n",
    "    Returns:\n",
    "    - float: the normalised Demarau-Levenshtein edit distance of the input strings\n",
    "    \"\"\"\n",
    "    return damerau_levenshtein_distance(read.__str__()[:min(len(overlap),len(read))],overlap.__str__()[:min(len(overlap),len(read))])/min(len(overlap),len(read))\n",
    "\n",
    "def build_suffix_array(reads: list, min_suf_len: int = 3,do_time: bool = False) -> tuple:\n",
    "    if do_time: start = time.time()\n",
    "    suf_arr = []\n",
    "    for read in reads:\n",
    "        read += '$' + str(reads.index(read))\n",
    "        for i in range(len(read)-min_suf_len-1):\n",
    "            # if len(read[i:]) < min_suf_len + 2: continue \n",
    "            suf_arr += [read[i:]]\n",
    "    suf_arr.sort()\n",
    "    suf_arr_ind = []\n",
    "    for s in range(len(suf_arr)):\n",
    "        suf_arr_ind += [int(suf_arr[s].split('$')[-1].__str__())]\n",
    "        suf_arr[s] = suf_arr[s][:suf_arr[s].find('$')+1]\n",
    "    if do_time: return suf_arr, suf_arr_ind, time.time() - start\n",
    "    return suf_arr,suf_arr_ind\n",
    "\n",
    "def create_bipartite_adjacency_matrix(reads: list, suf_arr: list = None, suf_arr_ind: list = None, do_time: bool = False,max_diff: float = 0.25, min_suf_len: int = 3) -> dict:\n",
    "    if do_time: start = time.time()\n",
    "    elif suf_arr is None or suf_arr_ind is None: suf_arr,suf_arr_ind = build_suffix_array(reads,min_suf_len=min_suf_len)\n",
    "    reads_map = dict(zip(reads,list(range(len(reads)))))\n",
    "    B = {}\n",
    "    for read in reads:\n",
    "        for j in range(min_suf_len + 1):\n",
    "            i = suf_arr.index(read[j:]+'$') - 1\n",
    "            while normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) <= 0.5:\n",
    "                if not reads[suf_arr_ind[i]] == read and \\\n",
    "                   normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) < max_diff and \\\n",
    "                   read.startswith(suf_arr[i][:-1]):\n",
    "                    if (reads_map[reads[suf_arr_ind[i]]],reads_map[read]) not in B: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = len(suf_arr[i][:-1])\n",
    "                    else: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = max(len(suf_arr[i][:-1]),B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])])\n",
    "                i -= 1\n",
    "    if do_time: return B, time.time() - start\n",
    "    return B\n",
    "\n",
    "def move_col(B: coo_matrix, cols: dict) -> None:\n",
    "    for c in range(len(B.col)):\n",
    "        B.col[c] = cols[B.col[c]]\n",
    "            \n",
    "def move_row(B: coo_matrix,rows: dict) -> None:\n",
    "    for r in range(len(B.row)):\n",
    "        B.row[r] = rows[B.row[r]]\n",
    "\n",
    "def find_lower_diagonal_path(B: coo_matrix,reads_map: dict,cols: list,rows: list,do_time: bool = False) -> tuple:\n",
    "    if do_time: start = time.time()\n",
    "    argpen = lambda l: np.argpartition(l,-2)[-2]\n",
    "\n",
    "    new_cols = cols[:]\n",
    "    if B.sum(axis=0).min() == 0: new_cols = list(c for c in new_cols if c not in [new_cols[B.sum(axis=0).argmin()]]) + [new_cols[B.sum(axis=0).argmin()]]\n",
    "    if B.sum(axis=1).min() == 0: \n",
    "        if B.sum(axis=1).argmin() == B.sum(axis=0).argmin():\n",
    "            new_cols = [new_cols[-1]] + list(c for c in new_cols[:-1] if c not in [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]) + [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]\n",
    "        else: new_cols = [rows[B.sum(axis=1).argmin()]] + list(c for c in new_cols if c not in [rows[B.sum(axis=1).argmin()]])\n",
    "\n",
    "    cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "    move_col(B,cols_map)\n",
    "    cols = new_cols\n",
    "\n",
    "    new_rows = cols[:]\n",
    "    rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "    move_row(B,rows_map)\n",
    "    rows = new_rows\n",
    "\n",
    "    i,j,k = len(rows), len(cols) - 1, B.sum(axis=1).argmin() if B.sum(axis=1).min() == 0 else None\n",
    "\n",
    "    while j > (k if B.sum(axis=1).min() == 0 else 0):\n",
    "        if k is not None and B.getrow(rows.index(cols[j])).argmax() == k: \n",
    "            cols_,c_ = [], 0\n",
    "\n",
    "            while j + c_ + 1 < len(rows):\n",
    "                c_ += 1\n",
    "                if len(B.getrow(j+c_).nonzero()[1]) > 1:\n",
    "                    cols_ = np.argpartition(B.getrow(j+c_).toarray().flatten(),-2)[::-1][:2]\n",
    "                    if cols[cols_[1]] in cols[:j] and B.getcol(cols_[1]).argmax() == j+c_: break\n",
    "            \n",
    "            if j + c_ + 1 == len(cols): new_cols = cols[:k+1] + cols[j:] + cols[k+1:j]\n",
    "            else: new_cols = cols[:k+1] + cols[j:j+c_] + list(c for c in cols[k+1:j] if c not in [cols[min(cols_)]]) + [cols[min(cols_)]] + cols[j+c_:]\n",
    "            cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "            move_col(B,cols_map)\n",
    "            cols = new_cols\n",
    "\n",
    "            if j + c_ + 1 == len(rows): new_rows = cols[:]\n",
    "            else: new_rows = cols[:k+c_+1] + list(r for r in rows[k:j+c_] if r not in cols[:k+c_+1] + cols[j+c_:]) + cols[j+c_:]\n",
    "            rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "            move_row(B,rows_map)\n",
    "            rows = new_rows\n",
    "\n",
    "            i,j,k = j + c_ + 1, j + c_, k + c_\n",
    "        else:\n",
    "            cmax = B.getrow(rows.index(cols[j])).argmax()\n",
    "            if len(B.getrow(rows.index(cols[j])).nonzero()[1]) > 1:\n",
    "                cpen = argpen(B.getrow(rows.index(cols[j])).toarray().flatten()) \n",
    "                if cmax > j: \n",
    "                    if len(B.getrow(cmax+1).nonzero()[1]) > 1 and \\\n",
    "                    B.getrow(cmax+1).getcol(argpen(B.getrow(cmax+1).toarray().flatten())).data[0] >=  B.getrow(rows.index(cols[j])).getcol(cpen).data[0]: \n",
    "                        crange = [argpen(B.getrow(cmax).toarray().flatten()),cmax]\n",
    "                    else: crange = [cpen]\n",
    "                else: crange = [cmax]\n",
    "            else: crange = [cmax]\n",
    "            while crange[0] > j:\n",
    "                if len(B.getrow(crange[0]).nonzero()[1]) > 1:\n",
    "                    crange = [argpen(B.getrow(crange[0]).toarray().flatten())] + crange\n",
    "                else:\n",
    "                    crange = [B.getrow(crange[0]).argmax()] + crange\n",
    "                if crange[0] == j: crange = [B.getrow(crange[1]).argmax()] + crange[1:]\n",
    "\n",
    "            new_cols = list(c for c in cols[:j] if c not in list(cols[cr] for cr in crange)) + list(cols[cr] for cr in crange) + list(c for c in cols[j:] if c not in list(cols[cr] for cr in crange))\n",
    "            cols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "            move_col(B,cols_map)\n",
    "            cols = new_cols\n",
    "\n",
    "            new_rows = list(r for r in rows[:i] if r not in cols[j:]) + cols[j:]\n",
    "            rows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "            move_row(B,rows_map)\n",
    "            rows = new_rows\n",
    "        j -= 1\n",
    "        i -= 1\n",
    "\n",
    "    seq = ''\n",
    "    for s,d in zip(list(reads_map[k] for k in rows)[:-1],B.diagonal(-1)):\n",
    "        seq += s[:-d]\n",
    "    seq += list(reads_map[k] for k in rows)[-1]\n",
    "    if do_time: return seq, time.time() - start\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "READS = [['betty_bought_butter_th',\n",
    "                         'tter_the_butter_was_',\n",
    "                               'he_butter_was_bitter_',\n",
    "                                          'as_bitter_betty_bought',\n",
    "                                                      'tty_bought_better_butter_t',\n",
    "                                                            'ught_better_butter_to',\n",
    "                                                                      'r_butter_to_make_the_',\n",
    "                                                                                    'ke_the_bitter_butter_better'],\n",
    "         ['you say hel',\n",
    "               'say hello wo',\n",
    "                      'lo world, i be',\n",
    "                            'ld, i bellow go t',\n",
    "                                      'ow go to hell'],\n",
    "         ['she_sells_s',\n",
    "                'lls_sea_shel',\n",
    "                     'ea_shells_o',\n",
    "                        'shells_on_the_s',\n",
    "                                   'he_sea_s',\n",
    "                                       'ea_shore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 1\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_1_sequitur__.csv','a') as f:\n",
    "    f.write('natural_language_sequence,edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "    reads = ['betty_bought_butter_th',\n",
    "                            'tter_the_butter_was_',\n",
    "                                'he_butter_was_bitter_',\n",
    "                                        'as_bitter_betty_bought',\n",
    "                                                    'tty_bought_better_butter_t',\n",
    "                                                        'ught_better_butter_to',\n",
    "                                                                    'r_butter_to_make_the_',\n",
    "                                                                                'ke_the_bitter_butter_better']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('1,{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_1_euler__.csv','a') as f:\n",
    "    f.write('natural language_sequence,k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better'\n",
    "    reads = ['betty_bought_butter_th',\n",
    "                            'tter_the_butter_was_',\n",
    "                                'he_butter_was_bitter_',\n",
    "                                        'as_bitter_betty_bought',\n",
    "                                                    'tty_bought_better_butter_t',\n",
    "                                                        'ught_better_butter_to',\n",
    "                                                                    'r_butter_to_make_the_',\n",
    "                                                                                'ke_the_bitter_butter_better']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('1,{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 54\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \n\u001b[1;32m---> 54\u001b[0m     graph \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meulerize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39meulerian_path(graph):\n",
      "File \u001b[1;32m<class 'networkx.utils.decorators.argmap'> compilation 4:3\u001b[0m, in \u001b[0;36margmap_eulerize_1\u001b[1;34m(G, backend, **backend_kwargs)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\networkx\\utils\\decorators.py:90\u001b[0m, in \u001b[0;36mnot_implemented_for.<locals>._not_implemented_for\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (mval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m mval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_multigraph()) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     88\u001b[0m     dval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dval \u001b[38;5;241m==\u001b[39m g\u001b[38;5;241m.\u001b[39mis_directed()\n\u001b[0;32m     89\u001b[0m ):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXNotImplemented(errmsg)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g\n",
      "\u001b[1;31mNetworkXNotImplemented\u001b[0m: not implemented for directed type",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(a,b):\n\u001b[0;32m     47\u001b[0m     G,outputs_dbg_time[k] \u001b[38;5;241m=\u001b[39m create_de_bruijn_graph(k,reads,do_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 48\u001b[0m     outputs_seq[k],outputs_euler_time[k] \u001b[38;5;241m=\u001b[39m \u001b[43meulerian_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdo_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m outputs_seq:\n\u001b[0;32m     50\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k,damerau_levenshtein_distance(outputs_seq[k],seq),\u001b[38;5;28mlen\u001b[39m(seq),\u001b[38;5;28mlen\u001b[39m(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[1;32mIn[10], line 64\u001b[0m, in \u001b[0;36meulerian_path\u001b[1;34m(graph, do_time)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(path)\n\u001b[1;32m---> 64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\A0068449\\Documents\\GitHub\\Sequitur\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# natural language sequence 2\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_2_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'you say hello world, i bellow go to hell'\n",
    "    reads = ['you say hel',\n",
    "                ' say hello wo',\n",
    "                        'lo world, i be',\n",
    "                            'ld, i bellow go t',\n",
    "                                        'ow go to hell']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('1,{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_2_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'you say hello world, i bellow go to hell'\n",
    "    reads = ['you say hel',\n",
    "                ' say hello wo',\n",
    "                        'lo world, i be',\n",
    "                            'ld, i bellow go t',\n",
    "                                        'ow go to hell']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequence 3\n",
    "# sequitur\n",
    "\n",
    "seed = 0\n",
    "n = 1000\n",
    "with open('data/output/natural_language_sequence_3_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    seq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "    reads = ['she_sells_s',\n",
    "                'lls_sea_shel',\n",
    "                        'ea_shells_o',\n",
    "                        'shells_on_the_s',\n",
    "                                    'he_sea_s',\n",
    "                                        'ea_shore']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(n):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\n",
    "# euler path\n",
    "with open('data/output/natural_language_sequence_3_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    seq = 'she_sells_sea_shells_on_the_sea_shore'\n",
    "    reads = ['she_sells_s',\n",
    "                'lls_sea_shel',\n",
    "                        'ea_shells_o',\n",
    "                        'shells_on_the_s',\n",
    "                                    'he_sea_s',\n",
    "                                        'ea_shore']\n",
    "    random.seed(seed)\n",
    "    random.shuffle(reads)\n",
    "    for _ in range(n):\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 generated sequences\n",
    "# sequitur\n",
    "n = 5\n",
    "m = 100\n",
    "for seed in range(n):  \n",
    "    with open('data/output/generated_sequence_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "        f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "        seq = generate_genome_sequence(10000,seed=seed)\n",
    "        reads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "        reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "        rows = list(range(len(reads)))\n",
    "        cols = list(range(len(reads)))\n",
    "        for _ in range(m):\n",
    "            suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "            B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "            start = time.time()\n",
    "            B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "            if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "            if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "            t2 += time.time() - start\n",
    "            seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "for seed in range(n): \n",
    "    with open('data/output/generated_sequence_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "        f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "        seq = generate_genome_sequence(10000,seed=seed)\n",
    "        reads,_ = generate_reads(seq,250,500,50,100,seed=seed)\n",
    "        outputs_seq = {}\n",
    "        outputs_dbg_time = {}\n",
    "        outputs_euler_time = {}\n",
    "        a,b = find_longest_overlap(reads)\n",
    "        for _ in range(m):\n",
    "            for k in range(a,b):\n",
    "                G,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "                outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "            for k in outputs_seq:\n",
    "                f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real genomic sequence, generated reads\n",
    "# sequitur\n",
    "n = 1\n",
    "m = 1\n",
    "seed = 37\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/sequitur/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "    f.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "    for record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "    reads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "    reads_map = dict(zip(list(range(len(reads))),reads))\n",
    "    rows = list(range(len(reads)))\n",
    "    cols = list(range(len(reads)))\n",
    "    for _ in range(m):\n",
    "        suf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "        B,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "        start = time.time()\n",
    "        B = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "        if B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "        if B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "        t2 += time.time() - start\n",
    "        if not os.path.exists('data/input/matrices/seed_'+str(seed)+'.npz'):\n",
    "            save_npz('data/input/matrices/seed_'+str(seed)+'.npz', B)\n",
    "        seq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "        f.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance2(str(seq_),str(seq),similarity=False),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/euler/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "    f.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "    for record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "    reads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "    outputs_seq = {}\n",
    "    outputs_dbg_time = {}\n",
    "    outputs_euler_time = {}\n",
    "    a,b = find_longest_overlap(list(str(read) for read in reads))\n",
    "    for _ in range(m):\n",
    "        for k in range(a,b):\n",
    "            G,outputs_dbg_time[k] = create_de_bruijn_graph(k,list(str(read) for read in reads),do_time=True)\n",
    "            outputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "        for k in outputs_seq:\n",
    "            f.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance2(outputs_seq[k],seq,similarity=False),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38731f125b301d8f0df7c54051f2a9a4c898c9398d16ef376d9fb7d661d33405"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
