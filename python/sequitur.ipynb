{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, random\n",
    "from typing import Union, Tuple\n",
    "from scipy.sparse import coo_matrix, vstack, hstack, save_npz, load_npz # pip install scipy\n",
    "import numpy as np # pip install numpy\n",
    "from math import ceil, fmod\n",
    "import networkx as nx # pip install networkx\n",
    "import pylcs # pip install pylcs\n",
    "from Bio import SeqIO # pip install Bio\n",
    "from Bio.Seq import Seq\n",
    "from fastDamerauLevenshtein import damerauLevenshtein as damerau_levenshtein_distance # pip install fastDamerauLevenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program: wgsim (short read simulator)\n",
      "Version: 1.20\n",
      "Contact: Heng Li <lh3@sanger.ac.uk>\n",
      "\n",
      "Usage:   wgsim [options] <in.ref.fa> <out.read1.fq> <out.read2.fq>\n",
      "\n",
      "Options: -e FLOAT      base error rate [0.020]\n",
      "         -d INT        outer distance between the two ends [500]\n",
      "         -s INT        standard deviation [50]\n",
      "         -N INT        number of read pairs [1000000]\n",
      "         -1 INT        length of the first read [70]\n",
      "         -2 INT        length of the second read [70]\n",
      "         -r FLOAT      rate of mutations [0.0010]\n",
      "         -R FLOAT      fraction of indels [0.15]\n",
      "         -X FLOAT      probability an indel is extended [0.30]\n",
      "         -S INT        seed for random generator [0, use the current time]\n",
      "         -A FLOAT      discard if the fraction of ambiguous bases higher than FLOAT [0.05]\n",
      "         -h            haplotype mode\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wgsim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[wgsim] seed = 420\n",
      "[wgsim_core] calculating the total length of the reference sequence...\n",
      "[wgsim_core] 1 sequences, total length: 258426\n"
     ]
    }
   ],
   "source": [
    "! wgsim -r 0 -R 0 -X 0 -e 0 -s 0 -S 420 -1 300 -2 300 '/workspaces/Sequitur/data/input/Raphanus sativus_NC_018551.1/Raphanus sativus_NC_018551.1.fasta' '/workspaces/Sequitur/data/input/Raphanus sativus_NC_018551.1/300.0.1.fastq' '/workspaces/Sequitur/data/input/Raphanus sativus_NC_018551.1/300.0.2.fastq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for record in SeqIO.parse(\"/workspaces/Sequitur/data/input/Raphanus sativus_NC_018551.1/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "any(seq.startswith(read.seq.upper()) or seq.startswith(read.seq.upper().reverse_complement()) for read in SeqIO.parse(f\"/workspaces/Sequitur/data/input/Raphanus sativus_NC_018551.1/300.0.2.fastq\",'fastq'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# De Bruijn Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation for a De Bruijn Graph assembler. In reality, many statistical, optimisation, and computation techniques are implemented to improve the efficiency and quality of input. Here we use a basic technique to compare to our basic technique. The idea being that with the addition of similar developments, the novel technique presented here might be able to at least match or perhaps surpass this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_longest_overlap(reads: list) -> int:\n",
    "\toverlaps = []\n",
    "\tfor read in reads:\n",
    "\t\toverlaps += pylcs.lcs2_of_list(read, list(set(reads).symmetric_difference([read])))\n",
    "\treturn min(overlaps),max(overlaps)\n",
    "\n",
    "def create_de_bruijn_graph(\n",
    "\t\tk: int, \n",
    "\t\tsequences: list,\n",
    "\t\tdo_time: bool = False\n",
    "    ) -> Union[nx.Graph,Tuple[nx.Graph,float]]:\n",
    "\t\"\"\"\n",
    "\tCreate a de Bruijn graph from a set of DNA sequences.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- k (int): k-mer size\n",
    "\t- sequences (list): List of DNA sequences\n",
    "\t- do_time (bool): should the function be time of not (default False)\n",
    "\t\n",
    "\tReturns:\n",
    "\t- Union[nx.Graph,Tuple[nx.Graph,float]]: De Bruijn graph or a tuple of the De Bruijn graph and the time\n",
    "\t\t\t\t\t\t\t\t\t\tto create it.\n",
    "\t\"\"\"\n",
    "\tif do_time: start_time = time.time()\n",
    "\tgraph = nx.DiGraph()\n",
    "\n",
    "\tfor sequence in sequences:\n",
    "\t\tfor i in range(len(sequence) - k + 1):\n",
    "\t\t\tkmer = sequence[i:i+k]\n",
    "\t\t\tprefix = kmer[:-1]\n",
    "\t\t\tsuffix = kmer[1:]\n",
    "\t\t\t\n",
    "\t\t\tif not graph.has_edge(prefix, suffix):\n",
    "\t\t\t\tgraph.add_edge(prefix, suffix, weight=1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tgraph[prefix][suffix]['weight'] += 1\n",
    "\n",
    "\tif do_time: return graph, time.time() - start_time\n",
    "\treturn graph\n",
    "\n",
    "def eulerian_path(\n",
    "    \tgraph: nx.DiGraph, \n",
    "     \tdo_time: bool = False\n",
    "\t) -> Union[str, Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Find an Eulerian path in the given graph or an approximate Eulerian path if none exists.\n",
    "    \n",
    "    Parameters:\n",
    "    - graph (nx.Graph): De Bruijn graph\n",
    "    - do_time (bool): should the function be timed or not (default False)\n",
    "    \n",
    "    Returns:\n",
    "    - Union[str, Tuple[str, float]]: string describing the Eulerian path or a tuple of this \n",
    "                                string and the execution time.\n",
    "    \"\"\"\n",
    "    if do_time: start_time = time.time()\n",
    "    path = []\n",
    "    \n",
    "    if nx.has_eulerian_path(graph):\n",
    "        for node in nx.eulerian_path(graph):\n",
    "            if len(path): path.append(node[0][-1])\n",
    "            else: path.append(node[0])\n",
    "        \n",
    "        if len(node[1]): path.append(node[1][-1])\n",
    "        else: path.append(node[1])\n",
    "        \n",
    "        if do_time: return ''.join(path), time.time() - start_time\n",
    "        return ''.join(path)\n",
    "    else:\n",
    "        start_node = next((node for node in graph.nodes if graph.out_degree(node) > 0), None)\n",
    "        if start_node is None:\n",
    "            if do_time: return '', time.time() - start_time\n",
    "            return ''\n",
    "        \n",
    "        current_node = start_node\n",
    "        while True:\n",
    "            path.append(current_node)\n",
    "            neighbors = list(graph.successors(current_node))\n",
    "            if not neighbors:\n",
    "                break\n",
    "            next_node = neighbors[0]\n",
    "            graph.remove_edge(current_node, next_node)\n",
    "            current_node = next_node\n",
    "        \n",
    "        if do_time: return ''.join(path), time.time() - start_time\n",
    "        return ''.join(path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the methods necessary for implementing the Sequitur assembly technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalised_damerau_levenshtein_distance(read: str,overlap: str) -> float:\n",
    "\t\"\"\"\n",
    "\tFind the Damerau-Levenshtein edit distance of two strings normalised to the length\n",
    "\tof the shorter string. This normalisation is because we want to path prefixes to\n",
    "\tsuffixes and this means that in general we will be comparing a full string to a\n",
    "\tportion of another string.\n",
    "\t\n",
    "\tParameters:\n",
    "\t- read (str): string for comparison, usually the longer string \n",
    "\t- overlap (str): string for comparison, usually the shorter string\n",
    "\t\n",
    "\tReturns:\n",
    "\t- float: the normalised Demarau-Levenshtein edit distance of the input strings\n",
    "\t\"\"\"\n",
    "\treturn damerau_levenshtein_distance(read.__str__()[:min(len(overlap),len(read))],overlap.__str__()[:min(len(overlap),len(read))])/min(len(overlap),len(read))\n",
    "\n",
    "def build_suffix_array(reads: list, min_suf_len: int = 3,do_time: bool = False) -> tuple:\n",
    "\tif do_time: start = time.time()\n",
    "\tsuf_arr = []\n",
    "\tfor read in reads:\n",
    "\t\tread += '$' + str(reads.index(read))\n",
    "\t\tfor i in range(read.index('$')-min_suf_len-1):\n",
    "\t\t\t# if len(read[i:]) < min_suf_len + 2: continue \n",
    "\t\t\tsuf_arr += [read[i:]]\n",
    "\tsuf_arr.sort()\n",
    "\tsuf_arr_ind = []\n",
    "\tfor s in range(len(suf_arr)):\n",
    "\t\tsuf_arr_ind += [int(suf_arr[s].split('$')[-1].__str__())]\n",
    "\t\tsuf_arr[s] = suf_arr[s][:suf_arr[s].find('$')+1]\n",
    "\tif do_time: return suf_arr, suf_arr_ind, time.time() - start\n",
    "\treturn suf_arr,suf_arr_ind\n",
    "\n",
    "def create_bipartite_adjacency_matrix(reads: list, suf_arr: list = None, suf_arr_ind: list = None, do_time: bool = False,max_diff: float = 0.25, min_suf_len: int = 3) -> dict:\n",
    "\tif do_time: start = time.time()\n",
    "\tif suf_arr is None or suf_arr_ind is None: suf_arr,suf_arr_ind = build_suffix_array(reads,min_suf_len=min_suf_len)\n",
    "\treads_map = dict(zip(reads,list(range(len(reads)))))\n",
    "\tB = {}\n",
    "\tfor read in reads:\n",
    "\t\tfor j in range(min_suf_len + 1):\n",
    "\t\t\ti = suf_arr.index(read[j:]+'$') - 1\n",
    "\t\t\twhile normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) <= 0.5:\n",
    "\t\t\t\tif not reads[suf_arr_ind[i]] == read and \\\n",
    "\t\t\t\t   normalised_damerau_levenshtein_distance(read,suf_arr[i][:-1]) < max_diff and \\\n",
    "\t\t\t\t   read.startswith(suf_arr[i][:-1]):\n",
    "\t\t\t\t\tif (reads_map[reads[suf_arr_ind[i]]],reads_map[read]) not in B: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = len(suf_arr[i][:-1])\n",
    "\t\t\t\t\telse: B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])] = max(len(suf_arr[i][:-1]),B[(reads_map[reads[suf_arr_ind[i]]],reads_map[read])])\n",
    "\t\t\t\ti -= 1\n",
    "\tif do_time: return B, time.time() - start\n",
    "\treturn B\n",
    "\n",
    "def move_col(B: coo_matrix, cols: dict) -> None:\n",
    "\tfor c in range(len(B.col)):\n",
    "\t\tB.col[c] = cols[B.col[c]]\n",
    "\t\t\t\n",
    "def move_row(B: coo_matrix,rows: dict) -> None:\n",
    "\tfor r in range(len(B.row)):\n",
    "\t\tB.row[r] = rows[B.row[r]]\n",
    "\n",
    "def find_lower_diagonal_path(B: coo_matrix,reads_map: dict,cols: list,rows: list,do_time: bool = False) -> tuple:\n",
    "\tif do_time: start = time.time()\n",
    "\targpen = lambda l: np.argpartition(l,-2)[-2]\n",
    "\n",
    "\tnew_cols = cols[:]\n",
    "\tif B.sum(axis=0).min() == 0: new_cols = list(c for c in new_cols if c not in [new_cols[B.sum(axis=0).argmin()]]) + [new_cols[B.sum(axis=0).argmin()]]\n",
    "\tif B.sum(axis=1).min() == 0: \n",
    "\t\tif B.sum(axis=1).argmin() == B.sum(axis=0).argmin():\n",
    "\t\t\tnew_cols = [new_cols[-1]] + list(c for c in new_cols[:-1] if c not in [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]) + [cols[B.getrow(rows.index(new_cols[-1])).argmax()]]\n",
    "\t\telse: new_cols = [rows[B.sum(axis=1).argmin()]] + list(c for c in new_cols if c not in [rows[B.sum(axis=1).argmin()]])\n",
    "\n",
    "\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\tmove_col(B,cols_map)\n",
    "\tcols = new_cols\n",
    "\n",
    "\tnew_rows = cols[:]\n",
    "\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\tmove_row(B,rows_map)\n",
    "\trows = new_rows\n",
    "\n",
    "\ti,j,k = len(rows), len(cols) - 1, B.sum(axis=1).argmin() if B.sum(axis=1).min() == 0 else None\n",
    "\n",
    "\twhile j > (k if B.sum(axis=1).min() == 0 else 0):\n",
    "\t\tif k is not None and B.getrow(rows.index(cols[j])).argmax() == k: \n",
    "\t\t\tcols_,c_ = [], 0\n",
    "\n",
    "\t\t\twhile j + c_ + 1 < len(rows):\n",
    "\t\t\t\tc_ += 1\n",
    "\t\t\t\tif len(B.getrow(j+c_).nonzero()[1]) > 1:\n",
    "\t\t\t\t\tcols_ = np.argpartition(B.getrow(j+c_).toarray().flatten(),-2)[::-1][:2]\n",
    "\t\t\t\t\tif cols[cols_[1]] in cols[:j] and B.getcol(cols_[1]).argmax() == j+c_: break\n",
    "\t\t\t\n",
    "\t\t\tif j + c_ + 1 == len(cols): new_cols = cols[:k+1] + cols[j:] + cols[k+1:j]\n",
    "\t\t\telse: new_cols = cols[:k+1] + cols[j:j+c_] + list(c for c in cols[k+1:j] if c not in [cols[min(cols_)]]) + [cols[min(cols_)]] + cols[j+c_:]\n",
    "\t\t\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\t\t\tmove_col(B,cols_map)\n",
    "\t\t\tcols = new_cols\n",
    "\n",
    "\t\t\tif j + c_ + 1 == len(rows): new_rows = cols[:]\n",
    "\t\t\telse: new_rows = cols[:k+c_+1] + list(r for r in rows[k:j+c_] if r not in cols[:k+c_+1] + cols[j+c_:]) + cols[j+c_:]\n",
    "\t\t\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\t\t\tmove_row(B,rows_map)\n",
    "\t\t\trows = new_rows\n",
    "\n",
    "\t\t\ti,j,k = j + c_ + 1, j + c_, k + c_\n",
    "\t\telse:\n",
    "\t\t\tcmax = B.getrow(rows.index(cols[j])).argmax()\n",
    "\t\t\tif len(B.getrow(rows.index(cols[j])).nonzero()[1]) > 1:\n",
    "\t\t\t\tcpen = argpen(B.getrow(rows.index(cols[j])).toarray().flatten()) \n",
    "\t\t\t\tif cmax > j: \n",
    "\t\t\t\t\tif len(B.getrow(cmax+1).nonzero()[1]) > 1 and \\\n",
    "\t\t\t\t\tB.getrow(cmax+1).getcol(argpen(B.getrow(cmax+1).toarray().flatten())).data[0] >=  B.getrow(rows.index(cols[j])).getcol(cpen).data[0]: \n",
    "\t\t\t\t\t\tcrange = [argpen(B.getrow(cmax).toarray().flatten()),cmax]\n",
    "\t\t\t\t\telse: crange = [cpen]\n",
    "\t\t\t\telse: crange = [cmax]\n",
    "\t\t\telse: crange = [cmax]\n",
    "\t\t\twhile crange[0] > j:\n",
    "\t\t\t\tif len(B.getrow(crange[0]).nonzero()[1]) > 1:\n",
    "\t\t\t\t\tcrange = [argpen(B.getrow(crange[0]).toarray().flatten())] + crange\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcrange = [B.getrow(crange[0]).argmax()] + crange\n",
    "\t\t\t\tif crange[0] == j: crange = [B.getrow(crange[1]).argmax()] + crange[1:]\n",
    "\n",
    "\t\t\tnew_cols = list(c for c in cols[:j] if c not in list(cols[cr] for cr in crange)) + list(cols[cr] for cr in crange) + list(c for c in cols[j:] if c not in list(cols[cr] for cr in crange))\n",
    "\t\t\tcols_map = dict((cols.index(c),new_cols.index(c)) for c in range(len(cols)))\n",
    "\t\t\tmove_col(B,cols_map)\n",
    "\t\t\tcols = new_cols\n",
    "\n",
    "\t\t\tnew_rows = list(r for r in rows[:i] if r not in cols[j:]) + cols[j:]\n",
    "\t\t\trows_map = dict((rows.index(r),new_rows.index(r)) for r in range(len(rows)))\n",
    "\t\t\tmove_row(B,rows_map)\n",
    "\t\t\trows = new_rows\n",
    "\t\tj -= 1\n",
    "\t\ti -= 1\n",
    "\n",
    "\tseq = ''\n",
    "\tfor s,d in zip(list(reads_map[k] for k in rows)[:-1],B.diagonal(-1)):\n",
    "\t\tseq += s[:-d]\n",
    "\tseq += list(reads_map[k] for k in rows)[-1]\n",
    "\tif do_time: return seq, time.time() - start\n",
    "\treturn seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# natural language sequences\n",
    "nat_lang_seq = [\n",
    "\t\t('betty_bought_butter_the_butter_was_bitter_betty_bought_better_butter_to_make_the_bitter_butter_better',\n",
    "\t\t['betty_bought_butter_th',\n",
    "\t\t\t\t\t\t\t'tter_the_butter_was_',\n",
    "\t\t\t\t\t\t\t\t'he_butter_was_bitter_',\n",
    "\t\t\t\t\t\t\t\t\t\t'as_bitter_betty_bought',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t'tty_bought_better_butter_t',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ught_better_butter_to',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'r_butter_to_make_the_',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'ke_the_bitter_butter_better']),\n",
    "\t\t('you say hello world, i bellow go to hell',\n",
    "\t\t['you say hel',\n",
    "\t\t\t\t\t' say hello wo',\n",
    "\t\t\t\t\t\t\t'lo world, i be',\n",
    "\t\t\t\t\t\t\t\t'ld, i bellow go t',\n",
    "\t\t\t\t\t\t\t\t\t\t\t'ow go to hell']),\n",
    "\t\t('she_sells_sea_shells_on_the_sea_shore',\n",
    "\t\t['she_sells_s',\n",
    "\t\t\t\t\t'lls_sea_shel',\n",
    "\t\t\t\t\t\t\t'ea_shells_o',\n",
    "\t\t\t\t\t\t\t'shells_on_the_s',\n",
    "\t\t\t\t\t\t\t\t\t\t'he_sea_s',\n",
    "\t\t\t\t\t\t\t\t\t\t\t'ea_shore'])\n",
    "]\n",
    "for i, (seq, reads) in enumerate(nat_lang_seq):\n",
    "\tif not os.path.exists('data/output/natural_language_sequences.sequitur.csv') or os.path.getsize('data/output/natural_language_sequences.sequitur.csv') == 0:\n",
    "\t\twith open('data/output/natural_language_sequences.sequitur.csv', 'a') as f:\n",
    "\t\t\tf.write('natural_language_sequence,edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\twith open('data/output/natural_language_sequences.sequitur.csv','a') as f:\n",
    "\t\tfor seed in range(10):\n",
    "\t\t\trandom.seed(seed)\n",
    "\t\t\trandom.shuffle(reads)\n",
    "\t\t\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\t\t\trows = list(range(len(reads)))\n",
    "\t\t\tcols = list(range(len(reads)))\n",
    "\t\t\tfor _ in range(100):\n",
    "\t\t\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\t\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\t\t\tstart = time.time()\n",
    "\t\t\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\t\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1, B.shape[1]),dtype=B.dtype)])\n",
    "\t\t\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\t\t\tt2 += time.time() - start\n",
    "\t\t\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\t\t\tf.write('{},{},{},{},{},{},{}\\n'.format(i,damerau_levenshtein_distance(seq_,seq),len(seq),len(seq_),t1,t2,t3))\n",
    "\tif not os.path.exists('data/output/natural_language_sequences.euler.csv') or os.path.getsize('data/output/natural_language_sequences.euler.csv') == 0:\n",
    "\t\twith open('data/output/natural_language_sequences.euler.csv', 'a') as f:\n",
    "\t\t\tf.write('natural language_sequence,k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\twith open('data/output/natural_language_sequences.euler.csv','a') as f:\n",
    "\t\tfor seed in range(10):\n",
    "\t\t\trandom.seed(seed)\n",
    "\t\t\trandom.shuffle(reads)\n",
    "\t\t\tfor _ in range(100):\n",
    "\t\t\t\toutputs_seq = {}\n",
    "\t\t\t\toutputs_dbg_time = {}\n",
    "\t\t\t\toutputs_euler_time = {}\n",
    "\t\t\t\ta,b = find_longest_overlap(reads)\n",
    "\t\t\t\tfor k in range(a,b):\n",
    "\t\t\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,reads,do_time=True)\n",
    "\t\t\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\t\t\tfor k in outputs_seq:\n",
    "\t\t\t\t\tf.write('{},{},{},{},{},{},{}\\n'.format(i,k,damerau_levenshtein_distance(outputs_seq[k],seq),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real genomic sequence, generated reads\n",
    "# sequitur\n",
    "n = 1\n",
    "m = 1\n",
    "seed = 37\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/sequitur/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_sequitur.csv','a') as f:\n",
    "\tf.write('edit_distance,target_sequence_length,output_sequence_length,suffix_array_construction_time,adjacency_matrix_construction_time,sequence_reconstruction_time\\n')\n",
    "\tfor record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "\treads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "\treads_map = dict(zip(list(range(len(reads))),reads))\n",
    "\trows = list(range(len(reads)))\n",
    "\tcols = list(range(len(reads)))\n",
    "\tfor _ in range(m):\n",
    "\t\tsuf_arr,suf_arr_ind,t1 = build_suffix_array(reads,do_time=True)\n",
    "\t\tB,t2 = create_bipartite_adjacency_matrix(reads,suf_arr=suf_arr,suf_arr_ind=suf_arr_ind,do_time=True)\n",
    "\t\tstart = time.time()\n",
    "\t\tB = coo_matrix((list(B.values()),list(zip(*B.keys())))).T\n",
    "\t\tif B.shape[0] < len(rows): B = vstack([B,coo_matrix((1,B.shape[1]),dtype=B.dtype)])\n",
    "\t\tif B.shape[1] < len(cols): B = hstack([B,coo_matrix((B.shape[0], 1),dtype=B.dtype)])\n",
    "\t\tt2 += time.time() - start\n",
    "\t\tif not os.path.exists('data/input/matrices/seed_'+str(seed)+'.npz'):\n",
    "\t\t\tsave_npz('data/input/matrices/seed_'+str(seed)+'.npz', B)\n",
    "\t\tseq_,t3 = find_lower_diagonal_path(B,reads_map,cols,rows,do_time=True)\n",
    "\t\tf.write('{},{},{},{},{},{}\\n'.format(damerau_levenshtein_distance2(str(seq_),str(seq),similarity=False),len(seq),len(seq_),t1,t2,t3))\n",
    "# euler\n",
    "# for seed in range(n):\n",
    "with open('data/output/local/real sequence/euler/Raphanus sativus_NC_018551.1_seed_'+str(seed)+'_euler.csv','a') as f:\n",
    "\tf.write('k,edit_distance,target_sequence_length,output_sequence_length,de_bruijn_graph_construction_time,euler_path_reconstruction_time\\n')\n",
    "\tfor record in SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1.fasta\",'fasta'): seq = record.seq\n",
    "\treads,_ = generate_reads(seq,250,250,50,50,seed=seed,min_coverage=None)\n",
    "\toutputs_seq = {}\n",
    "\toutputs_dbg_time = {}\n",
    "\toutputs_euler_time = {}\n",
    "\ta,b = find_longest_overlap(list(str(read) for read in reads))\n",
    "\tfor _ in range(m):\n",
    "\t\tfor k in range(a,b):\n",
    "\t\t\tG,outputs_dbg_time[k] = create_de_bruijn_graph(k,list(str(read) for read in reads),do_time=True)\n",
    "\t\t\toutputs_seq[k],outputs_euler_time[k] = eulerian_path(G,do_time=True)\n",
    "\t\tfor k in outputs_seq:\n",
    "\t\t\tf.write('{},{},{},{},{},{}\\n'.format(k,damerau_levenshtein_distance2(outputs_seq[k],seq,similarity=False),len(seq),len(outputs_seq[k]),outputs_dbg_time[k],outputs_euler_time[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "records1 = SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1/50.0.1.fastq\",'fastq')\n",
    "records2 = SeqIO.parse(\"data/input/Raphanus sativus_NC_018551.1/50.0.2.fastq\",'fastq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = (next(records1),next(records2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record[1].letter_annotations[\"phred_quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(record[0].seq.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(record[1].seq.upper())\n",
    "print(record[1].seq.upper().complement())\n",
    "print(record[1].seq.upper().reverse_complement())\n",
    "print(record[1].seq.upper()[::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '0123456$'\n",
    "s.index('$')-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
